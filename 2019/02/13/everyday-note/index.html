<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">






  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">











  
  
  <link rel="stylesheet" href="/lib/fancybox/source/jquery.fancybox.css">




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  

  
    
    
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mina:300,300italic,400,400italic,700,700italic|Lato:300,300italic,400,400italic,700,700italic|Indie Flower:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext">
  






<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">

<link rel="stylesheet" href="/css/main.css?v=7.1.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/uploads/favicon-32x32-xia.png?v=7.1.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/uploads/favicon-16x16-xia.png?v=7.1.2">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.2" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: true,
    fancybox: true,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="每天记点小笔记我也可以拿第一">
<meta property="og:type" content="article">
<meta property="og:title" content="笔记本">
<meta property="og:url" content="http://yoursite.com/2019/02/13/everyday-note/index.html">
<meta property="og:site_name" content="Xia&#39;s Blog">
<meta property="og:description" content="每天记点小笔记我也可以拿第一">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://yoursite.com/2019/02/13/everyday-note/positon-embedding-1.jpg">
<meta property="og:image" content="http://yoursite.com/2019/02/13/everyday-note/position-embedding-2.jpg">
<meta property="og:image" content="http://yoursite.com/2019/02/13/everyday-note/moving-average.png">
<meta property="og:image" content="http://yoursite.com/2019/02/13/everyday-note/debiasing.png">
<meta property="og:image" content="http://yoursite.com/2019/02/13/everyday-note/code-cost-longshort.png">
<meta property="og:image" content="http://yoursite.com/2019/02/13/everyday-note/calculating-entropy.JPG">
<meta property="og:image" content="http://yoursite.com/2019/02/13/everyday-note/cross-entropy.JPG">
<meta property="og:image" content="http://yoursite.com/2019/02/13/everyday-note/cross-entropy-unsymmetrical.JPG">
<meta property="og:image" content="http://yoursite.com/2019/02/13/everyday-note/simpson-separated-note.png">
<meta property="og:image" content="http://yoursite.com/2019/02/13/everyday-note/word-word-co-occurrence-matrix.png">
<meta property="og:image" content="http://yoursite.com/2019/02/13/everyday-note/attention.JPG">
<meta property="og:image" content="http://yoursite.com/2019/02/13/everyday-note/unlinear.JPG">
<meta property="og:image" content="http://yoursite.com/2019/02/13/everyday-note/unlinear1.JPG">
<meta property="og:image" content="http://yoursite.com/2019/02/13/everyday-note/Composition.JPG">
<meta property="og:updated_time" content="2020-02-26T15:01:37.808Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="笔记本">
<meta name="twitter:description" content="每天记点小笔记我也可以拿第一">
<meta name="twitter:image" content="http://yoursite.com/2019/02/13/everyday-note/positon-embedding-1.jpg">





  
  
  <link rel="canonical" href="http://yoursite.com/2019/02/13/everyday-note/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>笔记本 | Xia's Blog</title>
  




  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-142746162-1"></script>
  <script>
    var host = window.location.hostname;
    if (host !== "localhost" || !true) {
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-142746162-1');
    }
  </script>









  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Xia's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于我</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  
    <div class="reading-progress-bar"></div>
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/02/13/everyday-note/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="欢乐一只虾">
      <meta itemprop="description" content="xia写的">
      <meta itemprop="image" content="/uploads/xis.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xia's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">笔记本

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-02-13 15:01:33" itemprop="dateCreated datePublished" datetime="2019-02-13T15:01:33+08:00">2019-02-13</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2020-02-26 23:01:37" itemprop="dateModified" datetime="2020-02-26T23:01:37+08:00">2020-02-26</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/日常笔记/" itemprop="url" rel="index"><span itemprop="name">日常笔记</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="post-meta-item-icon">
            <i class="fa fa-eye"></i>
             阅读次数： 
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">本文字数：</span>
                
                <span title="本文字数">34k</span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <hr>
<p>每天记点小笔记<br>我也可以拿第一</p>
<hr>
<a id="more"></a>
<h1 id="2020"><a href="#2020" class="headerlink" title="2020"></a>2020</h1><h2 id="2020-02"><a href="#2020-02" class="headerlink" title="2020-02"></a>2020-02</h2><h3 id="2020-02-26"><a href="#2020-02-26" class="headerlink" title="2020-02-26"></a>2020-02-26</h3><ul>
<li>英语中的介词：<ul>
<li>on vs. in<ul>
<li>on.1 一物以另一物为支撑或载体<ul>
<li>It’s not easy to skate on one foot.</li>
<li>The man climbed down on a rope.</li>
<li>Dinner is on me tonight.</li>
</ul>
</li>
<li>on.2 一物以另一物为工具<ul>
<li>On the bus.</li>
<li>He’s shouting on the intercome.</li>
</ul>
</li>
<li>on.3 表示动作、行为、食事物所指向的对象<ul>
<li>She’s cheating on me.</li>
<li>Work on your interview skills.</li>
<li>Keep an eye on the baby.</li>
</ul>
</li>
<li>on.4 表示某事正在发生或在发生的过程中<ul>
<li>On sale.</li>
<li>On a rool. (连续交好运)</li>
<li>He is on a coffee break.</li>
<li>On my way home.</li>
</ul>
</li>
<li>in.1 某事发生在某个范围之内（不超出某个范围）<ul>
<li>He’ll be with you in five minutes.</li>
<li>She ripped the sheet of paper in two.</li>
</ul>
</li>
<li>in.2 一物是另一物的组成部分<ul>
<li>We’ve made a couple of changes in your schedule.</li>
<li>She’s everything I’d want in a wife.</li>
</ul>
</li>
<li>in.3 浸泡在某物中，受某物影响<ul>
<li>I’m in a hurry.</li>
<li>You’re in a big trouble.</li>
<li>He asked in surprise</li>
</ul>
</li>
<li>in.4 以某种方式<ul>
<li>She spoke in a low whisper.</li>
<li>I had to speak to him in French.</li>
<li>Do not write in pencil on this test.</li>
</ul>
</li>
</ul>
</li>
<li>over<ul>
<li><ol>
<li>结束</li>
</ol>
</li>
<li><ol>
<li>覆盖</li>
</ol>
<ul>
<li>I put another blanket over the baby.</li>
<li>I’ve been looking all over for you.</li>
</ul>
</li>
<li><ol>
<li>从某一点到另一点</li>
</ol>
<ul>
<li>Over here!</li>
<li>I come over to help.</li>
<li>I’d prefer to talk about it over the phone.</li>
</ul>
</li>
<li><ol>
<li>胜过、超过、优于</li>
</ol>
<ul>
<li>The game is designed for children over 6 yours old.</li>
<li>I’m so over men. (我看透男人了。)</li>
<li>That would’ve put him over the edge.(这会把他逼急的)</li>
</ul>
</li>
</ul>
</li>
<li>off<ul>
<li>脱离、离开，可衍生为“一个事物脱离原来的位置或状态”<ul>
<li>Get your feet off my couch.</li>
<li>I finally take this picture off the wall.</li>
<li>It pissed me off. (脱离了原来平静的状态，变得生气了)</li>
<li>Get your hands off me.</li>
<li>Joe was clearly relieved to be off the topic of his ex-firlfriend.  </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2020-02-25"><a href="#2020-02-25" class="headerlink" title="2020-02-25"></a>2020-02-25</h3><ul>
<li>C++<ul>
<li>inline（内联函数），以内存空间换运行效率</li>
<li>模板的实例化和调用都可以分显式和隐式（但模板类没有显式调用）</li>
<li>模板实例化的好处</li>
<li>函数模板特化和函数重载，不同之处：<ul>
<li>模板有“惰性实例化”的特性</li>
<li>重载函数使用起来更麻烦，需要在各个源文件中包含重载函数的申明</li>
</ul>
</li>
<li>过度优化</li>
</ul>
</li>
</ul>
<h3 id="2020-02-23"><a href="#2020-02-23" class="headerlink" title="2020-02-23"></a>2020-02-23</h3><ul>
<li>C++<ul>
<li>父类子类指针的函数调用的注意事项（三点）。虚拟函数就是为了对“如果你以一个基础类指针指向一个衍生类对象，那么通过该指针，你只能访问基础类定义的成员函数”这条规则反其道而行之的设计。</li>
<li>explicit 关键字</li>
<li>#define 定制自己的日志输出符号</li>
</ul>
</li>
</ul>
<h3 id="2020-02-22"><a href="#2020-02-22" class="headerlink" title="2020-02-22"></a>2020-02-22</h3><ul>
<li>英语思维：<ul>
<li>语言的掌握是一种心理认知的过程，传统教学把则把语言当成一种普通的知识灌输</li>
<li>体验是语言生成的途径，体验是用来建立语言和现实生活中的联系</li>
<li>构建自己的词汇网络</li>
<li>英语在表述上之所以比汉语更加客观、更加具有实质性，而汉语更加从自己的主观感觉出发。</li>
</ul>
</li>
</ul>
<ul>
<li>C++:<ul>
<li>纯虚函数<ul>
<li>所在的类不能被实例化</li>
<li>这个方法必须在派生类中被实现</li>
</ul>
</li>
<li>返回指针和引用</li>
</ul>
</li>
</ul>
<h3 id="2020-02-21"><a href="#2020-02-21" class="headerlink" title="2020-02-21"></a>2020-02-21</h3><ul>
<li>模板的实例化 length_norm</li>
<li>plda 效率问题</li>
<li>xvector.cpp -&gt; k_extract_xvector_from_feature（）里面的流程</li>
<li>resource-manager  engine-interface 怎么耦合在一起</li>
<li>log的流程，为什么用这么多宏命令</li>
</ul>
<h3 id="2020-02-20"><a href="#2020-02-20" class="headerlink" title="2020-02-20"></a>2020-02-20</h3><ul>
<li><p>ctc-loss, WER/LER</p>
</li>
<li><p>香辣小土豆：</p>
<ul>
<li>要用蒸的</li>
</ul>
</li>
</ul>
<h3 id="2020-02-15"><a href="#2020-02-15" class="headerlink" title="2020-02-15"></a>2020-02-15</h3><blockquote>
<div title="失真"> distortion </div>


</blockquote>
<h3 id="2020-02-11"><a href="#2020-02-11" class="headerlink" title="2020-02-11"></a>2020-02-11</h3><ul>
<li><p>C++:</p>
<ul>
<li><p><a href="https://www.cnblogs.com/flylong0204/p/4731318.html" target="_blank" rel="noopener">C++ 类对象和类指针的区别</a></p>
</li>
<li><p>C++ 类成员两种“初始化”方式（构造函数后面冒号 或 在构造函数里面赋值）的区别</p>
</li>
<li><p>函数后面用 const 修饰</p>
</li>
</ul>
</li>
</ul>
<ul>
<li>炒土豆片的时候记得焖一会儿</li>
</ul>
<h3 id="2020-02-10"><a href="#2020-02-10" class="headerlink" title="2020-02-10"></a>2020-02-10</h3><ul>
<li>git 删除分支：git branch —d guoxia-dev 和 git branch —D guoxia-dev 的区别</li>
</ul>
<h3 id="2020-02-07"><a href="#2020-02-07" class="headerlink" title="2020-02-07"></a>2020-02-07</h3><ul>
<li>炒牛肉片的技巧：<ul>
<li>牛肉筋络多的地方适合做大块的牛肉，比如红烧牛肉和土豆牛肉；筋络少的做炒肉片</li>
<li>切断纹路</li>
<li>盐水泡15分钟</li>
<li>用水淀粉包裹</li>
<li>先只炒一分钟（热锅凉油）</li>
<li>小苏打（保持嫩滑），白胡椒（去腥），老抽（上色），淀粉加水，油（炒的时候不粘）</li>
<li>辣椒，姜片，洋葱，肉片，最后放青椒</li>
</ul>
</li>
</ul>
<h3 id="2020-02-06"><a href="#2020-02-06" class="headerlink" title="2020-02-06"></a>2020-02-06</h3><ul>
<li>grep 命令的常用参数：<ul>
<li>-c: 只输出匹配行的行数。</li>
<li>-h: 查询多文件时不显示文件名。</li>
<li>-I: 不区分大小写（只适用于单字符）。</li>
<li>-n: 显示匹配的行和行号。</li>
<li>-s: 不显示 不存在或无匹配文本 的错误信息。</li>
<li>-v: 反向匹配，即显示不包含匹配文本的所有行。</li>
<li>-R: 递归查询，即连同子目录中的文件一起查询。</li>
</ul>
</li>
</ul>
<h3 id="2020-02-04"><a href="#2020-02-04" class="headerlink" title="2020-02-04"></a>2020-02-04</h3><ul>
<li><p>c++ 的枚举类型</p>
</li>
<li><p>bash shell常用快捷键：</p>
<ul>
<li><code>ctrl + l</code> ：清屏</li>
<li><code>ctrl + r</code> ：搜索</li>
<li><code>alt + b/f</code> ： 回退/前进 一个单词</li>
</ul>
</li>
<li><p><code>git add .</code> 和 <code>git add -u</code> 还有 <code>git add -A</code> 的区别</p>
</li>
<li>git 项目迁移</li>
</ul>
<h2 id="2020-01"><a href="#2020-01" class="headerlink" title="2020-01"></a>2020-01</h2><h3 id="2020-01-21"><a href="#2020-01-21" class="headerlink" title="2020-01-21"></a>2020-01-21</h3><blockquote>
<div title="不断前进的，又进展的"> progressive </div>

</blockquote>
<h3 id="2020-01-19"><a href="#2020-01-19" class="headerlink" title="2020-01-19"></a>2020-01-19</h3><blockquote>
<div title="发声，表达，说话方式"> utterance </div>
<div title="静止的，不动的"> stationary </div>

</blockquote>
<ul>
<li>声音三要数：<ul>
<li>响度/振幅</li>
<li>音高/频率</li>
<li>音色/相位</li>
<li>以上都不是线性相关关系，因为人耳的的听觉特征并非完全线性。                                                                                                     </li>
</ul>
</li>
<li>c 程序的编译过程:<ul>
<li>预处理：宏替换、头文件包含内容替换</li>
<li>编译：预处理后的源文件转换成汇编代码</li>
<li>汇编：汇编代码转换成二进制文件（中间文件）</li>
<li>链接：将各个中间文件链接到一起，生成可执行文件                                                                                                                                                                                                      </li>
</ul>
</li>
</ul>
<h3 id="2020-01-15"><a href="#2020-01-15" class="headerlink" title="2020-01-15"></a>2020-01-15</h3><ul>
<li><p>linux 命令：</p>
<ul>
<li>eval: 可以将命令里面的引用替换掉（两次扫描）</li>
<li>vim 查找：<code>/要查找的内容</code>， 后面带 <code>\c</code> 表示大小写不敏感，<code>\C</code> 反之</li>
</ul>
</li>
<li><p>python:</p>
<ul>
<li>调用 linux 系统命令：<ul>
<li>os.system / os.popen()</li>
<li>subprocess.Popen()</li>
</ul>
</li>
<li>终止程序：<ul>
<li>os._exit()：直接终止</li>
<li>sys.exit()：抛出异常(SystemExit)</li>
</ul>
</li>
</ul>
</li>
<li><p>Markdown 行内代码用单点，代码块用三个点。</p>
</li>
</ul>
<h3 id="2020-01-13"><a href="#2020-01-13" class="headerlink" title="2020-01-13"></a>2020-01-13</h3><ul>
<li>git:<ul>
<li>git config —global push.default matching/simple:<ul>
<li>matching 表示 push 的时候push所有已存在的同名分支到远程仓库</li>
<li>simple 表示只 push 当前所在的分支</li>
<li>git 2.0 以前默认是matching，2.0 以后默认变成了simple</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2020-01-09"><a href="#2020-01-09" class="headerlink" title="2020-01-09"></a>2020-01-09</h3><ul>
<li>C/C++语法：<ul>
<li>void* 做返回值表示该函数可以表示任意类型的指针</li>
<li>C语言中的static关键字可以修饰变量和函数，其作用都是使其修饰的东西仅在当前文件中有用，如果static修饰函数中的局部变量则表示该变量的作用域只是当前函数。static的该功能可以被匿名命名空间取代。在C++中static有了不同的用法，所以推荐用匿名空间来做作用域的控制。</li>
<li>extern 关键字： 告诉编译器该变量是存在的，但不是在该文件当前位置之前声明的，去别的地方应该能找到（可能是该文件当前位置的后面，也可能是在别的文件）。</li>
</ul>
</li>
</ul>
<h3 id="2020-01-08"><a href="#2020-01-08" class="headerlink" title="2020-01-08"></a>2020-01-08</h3><ul>
<li>Linux 命令行结果写入到文件用 <code>&gt;</code></li>
</ul>
<h3 id="2020-01-07"><a href="#2020-01-07" class="headerlink" title="2020-01-07"></a>2020-01-07</h3><ul>
<li>conda 命令：<ul>
<li>添加源： conda config —add channels <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</a></li>
<li>conda config —add channels <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</a></li>
<li>conda config —set show_channel_urls yes</li>
<li>删除添加的源： conda config —remove-key channels</li>
<li>conda config —remove channels <a href="https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free</a></li>
</ul>
</li>
</ul>
<h3 id="2020-01-06"><a href="#2020-01-06" class="headerlink" title="2020-01-06"></a>2020-01-06</h3><ul>
<li>C++语法：<ul>
<li>#include 后面用&lt;&gt;,和用“”的区别</li>
<li>引用 vs. 指针<ul>
<li>不存在空引用。引用必须连接到一块合法的内存。</li>
<li>一旦引用被初始化为一个对象，就不能被指向到另一个对象。指针可以在任何时候指向到不同对象</li>
<li>引用必须在创建时被初始化。指针可以在任何时间被初始化。</li>
<li>指针传递参数的本质还是值传递（形参是实参的一个副本），就是这个“值”变成了一个地址。</li>
<li>指针在逻辑上是独立的，他只是一个普通的变量，只是这个变量的值是一个地址（而不是别的数据类型，如int）；引用在逻辑上不是独立的，它的存在具有依附性，所有引用必须在声明的时候初始化，且之后不能引用别的对象。</li>
</ul>
</li>
<li>const 用法<ul>
<li>修饰普通变量</li>
<li>修饰指针变量</li>
<li>参数传递和函数返回值</li>
<li>修饰成员函数</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2020-01-03"><a href="#2020-01-03" class="headerlink" title="2020-01-03"></a>2020-01-03</h3><ul>
<li><p>Linux 命令：</p>
<ul>
<li>pwd : 查看当前路径</li>
<li>df : 察看硬盘信息<ul>
<li>-h: 让各个数字变得可读性更高，例如把1021个字节写出1k。</li>
</ul>
</li>
</ul>
</li>
<li><p>gcc - make - makefile - CMake</p>
</li>
</ul>
<h1 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h1><h2 id="2019-12"><a href="#2019-12" class="headerlink" title="2019-12"></a>2019-12</h2><h3 id="2019-12-19"><a href="#2019-12-19" class="headerlink" title="2019-12-19"></a>2019-12-19</h3><blockquote>
<div title="荒谬的；荒诞"> absurd </div>
<div title="有形的；可触摸的"> tangible </div>
<div title="自满的；沾沾自喜的"> smug </div>
<div title="使迷惑；使糊涂"> confound </div>
<div title="很小的；小小的"> teeny </div>
<div title="不合理的；无理数"> irrational </div>
<div title="芥末"> mustard </div>
<div title="结语；收场白"> epilogue </div>


</blockquote>
<ul>
<li><a href="https://betterexplained.com/articles/a-visual-intuitive-guide-to-imaginary-numbers/" target="_blank" rel="noopener">A Visual, Intuitive Guide to Imaginary Numbers</a><ul>
<li>Really Understanding Negative Numbers<ul>
<li>Negatives were considered absurd in the 1700s.</li>
</ul>
</li>
<li>Enter Imaginary Numbers<ul>
<li>Imaginary numbers are a tool to describe the world, which are as normal as every other number.</li>
<li>$i^2 = -1$</li>
</ul>
</li>
<li>Visual Understanding of Negative And Complex Numbers<ul>
<li>$1\cdot x \cdot x = -1$ : What transformation x, when applied twice, turn 1 into -1?</li>
<li>rotation</li>
<li>Numbers are 2-dimensional<ul>
<li>$i$ is a “new imaginary dimension” to measure a number</li>
<li>$i$ (or $-i$) is what numbers “become” when rotated</li>
<li>Multiplying $i$ is a rotation by 90 degrees counter-clockwise, multiplying by $-i$ is a rotation by 90 degrees clockwise</li>
<li>Two rotations in eigher direction is -1: it brings us back into the “regular” dimensions of positive and negative numbers.</li>
</ul>
</li>
</ul>
</li>
<li>Finding Patterns<ul>
<li>negative numbers: 1,-1,1,-1,1…</li>
<li>imaginary numbers: 1,$i$,-1,-$i$,1…</li>
</ul>
</li>
<li>Understanding Complex Numbers<ul>
<li>how “big” is a complex number?   <ul>
<li>Size of $a + bi = \sqrt{a^2+b^2}$ </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2019-12-18"><a href="#2019-12-18" class="headerlink" title="2019-12-18"></a>2019-12-18</h3><ul>
<li>尤克里里：<ul>
<li>几弦几品，有些不同的地方能发出一样的声音</li>
<li>控制拨弦力度统一</li>
<li>1-A，2-E，3-C，4-G</li>
</ul>
</li>
</ul>
<h2 id="2019-11"><a href="#2019-11" class="headerlink" title="2019-11"></a>2019-11</h2><h3 id="2019-11-25"><a href="#2019-11-25" class="headerlink" title="2019-11-25"></a>2019-11-25</h3><ul>
<li><p>Expectation-Maximization, EM 算法的简单情况可以用三硬币模型来理解。</p>
</li>
<li><p>HMM 模型的3个基本问题：</p>
<ul>
<li>概率计算问题</li>
<li>学习问题</li>
<li>预测问题</li>
</ul>
</li>
</ul>
<h3 id="2019-11-21"><a href="#2019-11-21" class="headerlink" title="2019-11-21"></a>2019-11-21</h3><ul>
<li>Blog: <a href="https://medium.com/stellargraph/knowing-your-neighbours-machine-learning-on-graphs-9b7c3d0d5896" target="_blank" rel="noopener">Machine Learning on Graphs</a><ul>
<li>Types of networks<ul>
<li>Homogeneous VS. Heterogeneous</li>
<li>Static VS. Dynamic</li>
</ul>
</li>
<li>What can we learn from graphs<ul>
<li>Node Classification, also know as node attribute inference</li>
<li>Link Prediction</li>
<li>Community Detection</li>
<li>Graph Classification</li>
</ul>
</li>
</ul>
</li>
<li>tensorflow, static_rnn 和 dynamic_rnn 区别<ul>
<li>static_rnn 是把RNN展开，复制固定数量的 RNNCell；而 dynamic_rnn 是用while循环</li>
<li>因此，输入到静态 RNN 的序列长度必须完全相同；而动态的 RNN 不同 batch 之间的序列长度可以不同，但是同一个 batch 里长度必须相同。</li>
<li>static_rnn 接受的是大小为 &lt;序列长度, batchSize, 词向量大小&gt; 的 tensor; 而 dynamic_rnn 接受的是大小为 <batchsize，序列长度，词向量大小> 的 tensor</batchsize，序列长度，词向量大小></li>
</ul>
</li>
</ul>
<h3 id="2019-11-20"><a href="#2019-11-20" class="headerlink" title="2019-11-20"></a>2019-11-20</h3><blockquote>
<div title="兜帽"> hood </div>



</blockquote>
<ul>
<li><p>Python 生成器只能遍历一遍，它也实现了迭代协议，可以说是一种特殊的迭代器。</p>
</li>
<li><p>Python 多进程和多线程：</p>
<ul>
<li>由于GIL，Python 的多线程同时只能有一个线程占用CPU。</li>
<li>多线程用 threading.Thread()；多进程用 multiprocessing.Process()</li>
<li>CPU 密集型程序用多进程；I/O 密集型程序用多线程</li>
</ul>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/92017824" target="_blank" rel="noopener">浅谈 Transformer-based 模型中的位置表示</a></p>
<ul>
<li>原始 Transformer 使用如下公式生成固定的位置表示<ul>
<li><script type="math/tex; mode=display">PE(pos, 2i) = sin(\frac{pos}{10000^{2i/d_{model}}})</script></li>
<li><script type="math/tex; mode=display">PE(pos, 2i+1) = cos(\frac{pos}{10000^{2i/d_{model}}})</script></li>
<li>使用这个公式的原因，作者表示,用这个公式可以表示出不同位置的词之间的相对位置信息，因为 $PE_{p+k}$ 可以由 $PE_p$ 的线性方程表示”</li>
<li>但是，这个方法得到的位置信息表示没有方向性；而且这个性质会被 self-attention 机制破坏。</li>
<li><img src="/2019/02/13/everyday-note/positon-embedding-1.jpg" title="两个不同位置的表示向量的乘积和偏移量k之间的关系"></li>
<li><img src="/2019/02/13/everyday-note/position-embedding-2.jpg" title="经过自注意力层的KQV映射之后，上述位置信息会被破坏"></li>
</ul>
</li>
<li>后面的BERT和一些相关的模型没有用上述的固定的位置信息表示，而是采用了所谓的 “learned and fixed” 的可学习的位置嵌入，就说训练一个位置嵌入矩阵，大小为 $L_{max} \times d$</li>
<li>相对位置表示（Relative Position Representations, RPR）<ul>
<li>RNN 依靠其循环机制，结合 $t$ 时刻的输入和前一时刻的隐藏层状态 $h_{t-1}$ 计算出 $h_t$, 直接通过其顺序结构沿时间维度捕捉相对和绝对位置；而非RNN模型不需要顺序处理输入，则需要显式编码才能引入位置信息。</li>
<li>原 transformer 中的 Self-attention 机制如下</li>
<li><script type="math/tex; mode=display">e_{ij} = \frac{x_i W^Q(x_jW^K)^T}{\sqrt{d_k}}</script></li>
<li><script type="math/tex; mode=display">\alpha_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^{n}exp(e_{ik})}</script></li>
<li><script type="math/tex; mode=display">z_i = \sum_{j=1}^{n}\alpha_{ij}(x_jW^V)</script></li>
<li>RPR 不在输入时将位置嵌入与词嵌入向量相加，而是选择对上述 Self-attention 进行改动：</li>
<li><script type="math/tex; mode=display">e_{ij} = \frac{x_i W^Q(x_jW^K + a_{ij}^K)^T}{\sqrt{d_k}}</script></li>
<li><script type="math/tex; mode=display">\alpha_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^{n}exp(e_{ik})}</script></li>
<li><script type="math/tex; mode=display">z_i = \sum_{j=1}^{n}\alpha_{ij}(x_jW^V + a_{ij}^V)</script></li>
<li>其中 $a_{ij}^{K,V}$ (不是 $\alpha$) 的计算方式如下：</li>
<li><script type="math/tex; mode=display">a_{ij}^K = w_{clip(j-i,k)}^K</script></li>
<li><script type="math/tex; mode=display">a_{ij}^V = w_{clip(j-i,k)}^V</script></li>
<li><script type="math/tex; mode=display">clip(j-u,k) = max(-k, min(K,x))</script></li>
<li>如此，模型需要学习的就说相对位置表示 $w^K = (w_{-k}^K,…,w_k^K)\in \mathbb{R}^{(2k+1)\times d}$ ，同一层的 attention heads 之间共享，但是在不同层之间是不同的。</li>
<li>不过，论文在对机器翻译任务进行效率实验时发现，$a_{ij}^V$ 可能不是必要的。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2019-11-19"><a href="#2019-11-19" class="headerlink" title="2019-11-19"></a>2019-11-19</h3><blockquote>
<div title="求导"> differentiate </div>
<div title="导数"> derivative </div>


</blockquote>
<ul>
<li><p><a href="https://towardsdatascience.com/an-intuitive-understanding-of-the-lamb-optimizer-46f8c0ae4866" target="_blank" rel="noopener">An intuitive understanding of the LAMB optimizer</a></p>
<ul>
<li><p>Adam:</p>
<ul>
<li>Keep a moving average of the gradients and their variance:  </li>
<li><script type="math/tex; mode=display">m^{(t-1)}_w \leftarrow \beta_1 m^{(t)}_w + (1-\beta_1)\nabla_w L^{(t)}</script></li>
<li><script type="math/tex; mode=display">v^{(t-1)}_w \leftarrow \beta_2 v^{(t)}_w + (1-\beta_2)(\nabla_w L^{(t)})^2</script></li>
<li>A visualization of how much smoothing we get on a noisy dataset for different betas.<ul>
<li>small batch - the gradient might be noisier - higher $\beta$</li>
<li>large batch - less noise - less $\beta$<div style="width:70%"><img src="/2019/02/13/everyday-note/moving-average.png"></div></li>
</ul>
</li>
<li>Debiasing:</li>
<li><script type="math/tex; mode=display">\hat{m}_t = m_t/(1-\beta^t_1)</script></li>
<li><script type="math/tex; mode=display">\hat{v}_t = v_t/(1-\beta^t_2)</script><div style="width:40%; margin:0 auto"><img src="/2019/02/13/everyday-note/debiasing.png"></div></li>
<li>The final parameter update<ul>
<li><script type="math/tex; mode=display">w^{(t+1)} \leftarrow w^{(t)} - \eta \frac{\hat{m}_w}{\sqrt{\hat{v}_w}+\epsilon}</script></li>
<li>numerator means: for every parameter, take a in the direction of the gradient for that parameter</li>
<li>denominator means: normalize the step by its standard deviation</li>
</ul>
</li>
<li>The intuitive interpretation<ul>
<li>If the gradients are all pointing in different directions (high variance), we’ll take a small, cautious step.</li>
<li>If all the gradients are telling us to move in the same direction, the variance will be small, so we’ll take a bigger step in that direction.</li>
</ul>
</li>
</ul>
</li>
<li>LARS, <a href="https://arxiv.org/pdf/1708.03888.pdf" target="_blank" rel="noopener">Layerwise Adaptive Rate Scaling</a><ul>
<li>Trust ratio: the ratio between the nom of the layer weights and norm of gradients update.</li>
<li><script type="math/tex; mode=display">\lambda^l = \eta \times \frac{||w^l||}{||\nabla L(w^l)||}</script></li>
</ul>
</li>
<li>LAMB, Layer-wise Adaptive Moments optimizer for Batch training<ul>
<li>It makes a few small changes to LARS<ul>
<li><ol>
<li>If the numerator (r1 below) or denominator (r2 below) of the trust ratio is 0, then use 1 instead.</li>
</ol>
</li>
<li><ol>
<li>Fixing weight decay: in LARS, the denominator of the trust ratio is $|\nabla L|+\beta |W|$, whereas in LAMB it’s $|\nabla L + \beta W|$. this preserves more information.</li>
</ol>
</li>
<li><ol>
<li>Instead of using the SGD update rule, they use Adam update rule.</li>
</ol>
</li>
<li><ol>
<li>CLip the trust ratio at 10</li>
</ol>
</li>
</ul>
</li>
<li>So, the trust ratio in LAMB is</li>
<li><script type="math/tex; mode=display">r_1 = ||w_{t-1}^l||_2</script></li>
<li><script type="math/tex; mode=display">r_2 = ||\frac{\hat{m}_t^l}{\sqrt{\hat{v}_t^l+\epsilon}}+\lambda w_{t-1}^l||_2</script></li>
<li><script type="math/tex; mode=display">r = r_1/r_2</script></li>
<li><script type="math/tex; mode=display">\eta^l = r \times \eta</script></li>
<li><script type="math/tex; mode=display">w_t^l = w_{t-1}^l - \eta^l \times (\frac{\hat{m}_t^l}{\sqrt{\hat{v}_t^l+\epsilon}}+\lambda w_{t-1}^l)</script></li>
<li>$r_2$ it the norm of the Adam update rule with weight decay</li>
<li>$\eta^l$ is the layer-wise learning rate adjusted by the trust ratio</li>
<li>So, this method can be summarized as LARS applied to Adam</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2019-11-13"><a href="#2019-11-13" class="headerlink" title="2019-11-13"></a>2019-11-13</h3><ul>
<li>编程之美<ul>
<li>3.1 字符串移位包含问题<ul>
<li>解法一：循环查找，N很大时很慢</li>
<li>解法二：在字符串后面再复制一遍自身，然后直接查找</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2019-11-12"><a href="#2019-11-12" class="headerlink" title="2019-11-12"></a>2019-11-12</h3><ul>
<li>编程之美<ul>
<li>2.1 求二进制数中的1的个数<ul>
<li>解法一：循环用2除</li>
<li>解法二：循环位操作，向右移一位</li>
<li>解法三：只考虑1的位置，本身-1后和本身进行与操作，比如 $01000000 \&amp; (01000000-00000001) =01000000 \And 00111111 = 0$。 这样一直循环能把原二进制数中的1逐渐清空</li>
<li>解法四：如果位数少的话，用枚举</li>
</ul>
</li>
<li>2.3 寻找数组中出现次数最多的数字<ul>
<li>一般情况： 先排序后遍历查找</li>
<li>已知次数最多的数字占中数组的一半以上： 两个一对的删除掉不一样的两个数，最后剩下的就是占比一半以上的那个数字</li>
</ul>
</li>
<li>2.7 求最大公约数<ul>
<li>辗转相除法：$f(x, y) = f(y, y \mod x)$</li>
<li>辗转相减法：$f(x, y) = f(x-y, y)$ 如果输入$x&lt;y$, 就先替换位置。缺点是迭代次数较多，比如$f(1000000000,1)$情况下</li>
<li>位运算和减法：<ul>
<li>若$x, y$均为偶数，$f(x,y)=2\times f(x/2,y/2)=2\times f(x\gg 1,y\gg 1)$</li>
<li>若$x$为偶数，$y$为奇数，$f(x,y)=f(x/2,y)=f(x\gg 1,y)$</li>
<li>若$x$为奇数，$y$为偶数，$f(x,y)=f(x,y/2)=f(x,y\gg 1)$</li>
<li>若$x,y$均为奇数，$f(x,y)=f(x,x-y)$</li>
</ul>
</li>
</ul>
</li>
<li>2.8 找符合条件的整数（N*M结构每一位上都是0或者1）<ul>
<li>解法一：给定N，从小到大遍历M。搜索空间太大。</li>
<li>解法二：搜索N*M的符合要求的结果，即所有位置都是0或1的所有可能取值X，然后检查是否能被N整除，如果最终结构X有k为，则要搜索$2^k$次。</li>
<li>解法三：维护一个“余数信息数组”</li>
</ul>
</li>
<li>2.12 两数之和满足条件<ul>
<li>最巧妙做法：直接考虑两数之和的顺序。先对原数组进行排序，然后第0个元素和最后一个元素相加的结果就是所有可能的两数之和的中位数，接下来如果当前两数之和小于目标值则前面的下标加一，否则后面的下标加一，直到找到目标值。</li>
</ul>
</li>
<li>2.16 求数组中最长递增子序列<ul>
<li>解法一：根据无后效性，我只需要记住到当前位置为止的最长递增子序列，比如1, -1, 2, -3, 4, -5, 6为例，遍历到4后，我们不用关心之前的两个值具体是什么，因为这对后面找到6并没有什么直接的影响。要找到当前位置的最小值，只要遍历前面记录的最长递增子序列然后满足比当前值小就行。</li>
<li>解法二：在解法1的基础上再记录长度为$i$的递增子序列最大元素的最小值$MaxV[1]$，这样能减少内层遍历的次数。</li>
</ul>
</li>
<li>2.19 区间重合判断<ul>
<li>解法一：遍历无序的目标区间，对原始区间进行覆盖，看最后有没有剩余没覆盖到的区间</li>
<li>解法二：对无序的目标区间进行排序并合并连续的区间，然后用二分查找寻找原始区间有没有被其中一个不变连续的区间包含</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2019-11-11"><a href="#2019-11-11" class="headerlink" title="2019-11-11"></a>2019-11-11</h3><ul>
<li>leetcode #72: 编辑距离<ul>
<li>动态规划：对于 $word1$ 中的第$i$个字母和单词 $word2$ 中的第$j$个字母，如果他们俩一样，则：$D[i][j] =1 + min(D[i-1][j],  D[i][j-1], D[i-1][j-1] - 1)$， 如果他们俩不一样，则：$D[i][j] = 1 + min(D[i-1][j],  D[i][j-1], D[i-1][j-1])$, 其中 $D[i][j]$ 表示 $word1[:i]$ 和 $word2[:j]$ 之间的最短编辑距离。</li>
</ul>
</li>
</ul>
<h2 id="2019-10"><a href="#2019-10" class="headerlink" title="2019-10"></a>2019-10</h2><h3 id="2019-10-31"><a href="#2019-10-31" class="headerlink" title="2019-10-31"></a>2019-10-31</h3><ul>
<li>Medium: <a href="https://medium.com/@_init_/an-illustrated-explanation-of-using-skipgram-to-encode-the-structure-of-a-graph-deepwalk-6220e304d71b" target="_blank" rel="noopener">An Illustrated Explanation of Using SkipGram To Encode The Structure of A Graph (DeepWalk)</a><ul>
<li>Generating corpus by random walk </li>
</ul>
</li>
</ul>
<h3 id="2019-10-27"><a href="#2019-10-27" class="headerlink" title="2019-10-27"></a>2019-10-27</h3><ul>
<li>LeetCode 戳气球问题</li>
</ul>
<h3 id="2019-10-17"><a href="#2019-10-17" class="headerlink" title="2019-10-17"></a>2019-10-17</h3><blockquote>
<div title="无处不在的"> ubiquitous </div>
<div title="伪代码"> pseudo code </div>
<div title="备用的"> auxiliary </div>

</blockquote>
<h3 id="2019-10-10"><a href="#2019-10-10" class="headerlink" title="2019-10-10"></a>2019-10-10</h3><blockquote>
<div title="难处理地；难解决的"> intractable </div>
<div title="卖弄学问的"> pedantic </div>


</blockquote>
<ul>
<li>why maximum likelihood and not maximun probability<ul>
<li>$L(\mu, \sigma; data)=P(data;\mu, \sigma)$</li>
<li>Despite these two things being equal, the likelihood and the probability density are fundamentally asking different questions — one is asking about the data and the other is asking about the parameter values. </li>
</ul>
</li>
</ul>
<h2 id="2019-09"><a href="#2019-09" class="headerlink" title="2019-09"></a>2019-09</h2><h3 id="2019-09-29"><a href="#2019-09-29" class="headerlink" title="2019-09-29"></a>2019-09-29</h3><blockquote>
<ul>
<li><div title="不变性，恒定性"> invariance </div>

</li>
</ul>
</blockquote>
<h3 id="2019-09-27"><a href="#2019-09-27" class="headerlink" title="2019-09-27"></a>2019-09-27</h3><ul>
<li>用 weight normalization 或者 batch normallization 相当于给原始神经网络引入了一个不同的参数化过程。batch normalization就不是没有 re-parameterization 过程。</li>
</ul>
<h3 id="2019-09-26"><a href="#2019-09-26" class="headerlink" title="2019-09-26"></a>2019-09-26</h3><ul>
<li>paper: ALBERT<ul>
<li>三大改造：<ul>
<li>将词嵌入矩阵与后面的隐藏层脱离，这就可以使模型的隐藏层大小远远大于词嵌入向量的大小而不影响模型效果。</li>
<li>跨层参数共享，同时共享FFN层参数和Self-attention层的参数。结果是，每一层输入与输出嵌入矩阵间的L2距离与余弦相似度都变得平滑很多。</li>
<li>引入句间连贯性损失（SOP）。避免预测主题，只关注建模句子之间的连贯性。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2019-09-25"><a href="#2019-09-25" class="headerlink" title="2019-09-25"></a>2019-09-25</h3><blockquote>
<ul>
<li><div title="使减小；使变少；变少">diminish</div></li>
<li><div title="正交的，直角的">orthogonal</div></li>
<li><div title="话虽如此"> That being said </div></li>
<li><div title="本质，精华"> essence </div></li>
<li><div title="初步的"> preliminary </div>


</li>
</ul>
</blockquote>
<ul>
<li><p>The difference between <code>xrange</code> and <code>range</code> in Python:</p>
<ul>
<li><code>range</code> returns a Python  <code>list</code> object</li>
<li><code>xrange</code> returns an <code>xrange</code> object witch is a <code>generators</code></li>
</ul>
</li>
<li><p>paper: Layer Normalization</p>
<ul>
<li>Problem of Batch Normalization:<ul>
<li>the effect is dependent on the mini-batch size</li>
<li>it is not obvious how to apply it to recurrent neural networks.</li>
</ul>
</li>
<li>Compare with Batch Normalization:<ul>
<li>Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity.</li>
<li>Unlike batch normalization, layer normalization performes exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step.</li>
</ul>
</li>
<li>layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. </li>
</ul>
</li>
</ul>
<h3 id="2019-09-24"><a href="#2019-09-24" class="headerlink" title="2019-09-24"></a>2019-09-24</h3><ul>
<li><p>排序算法的稳定性：</p>
<ul>
<li>稳定：冒泡、归并、插入、基数</li>
<li>不稳定：堆排序、快速、直接选择、希尔</li>
</ul>
</li>
<li><p>KNN 和 K-Means</p>
</li>
</ul>
<table width="80%" style="table-layout: fixed; margin:auto"> 
<tr>
<th>KNN</th>
<th>K-Means</th>
</tr>
<tr>
<td>1.分类算法 <br> 2.监督学习</td>
<td>1.聚类算法 <br> 2.非监督算法</td>
</tr>
<tr>
<td>没有明显的训练过程，只需要将所有数据载入到内存中，就可以对新输入的数据进行分类</td>
<td>有明显的训练过程，需要通过不停迭代找出最合适的K个平均值</td>
</tr>
<tr>
<td>K的意思是根据距离新数据最近的K个样本来对新数据进行分类</td>
<td>K的意思是假设所有的数据集中有K个种类，需要初始假设K个均值</td>
</tr>
<tr>
<td colspan="2">相似点：两者都用到了NN（Nearest Neighbor）思想，一般用KD树来实现</td>
</tr>
</table>

<h3 id="2019-09-23"><a href="#2019-09-23" class="headerlink" title="2019-09-23"></a>2019-09-23</h3><ul>
<li>论文笔记：核心思想、预备知识、模型、实验</li>
<li>transformer 中的 Add &amp; Norm 层：<ul>
<li>Add 层类似于残差连接，将自注意力层的输出与自身向量相加，让模型只需要在自注意力层学习增量信息，优点是可以学习到一些很细节的信息。</li>
<li>Norm 层用的是 layer normalization，与 batch normalization 相比优点是针对单样本，不依赖其它数据，可以避免 interval co-variate shift。</li>
<li>Feed-Forward Network：两层线性转换和一个激活层（RELU).</li>
</ul>
</li>
</ul>
<h3 id="2019-09-10"><a href="#2019-09-10" class="headerlink" title="2019-09-10"></a>2019-09-10</h3><ul>
<li>Kullback-Leibler Divergence &amp; Entropy<ul>
<li>entropy: it’s a way to quantify exactly how much information is in our data</li>
<li>KL divergence: a way to quantify how much information is lost from true distribution to parameterized approximation.</li>
<li>With KL divergence we can calculate exactly how much information is lost when we approximate one distribution with another.</li>
<li>Divergence not distance. The reason is that KL Divergence is not symmetric.</li>
</ul>
</li>
</ul>
<h3 id="2019-09-09"><a href="#2019-09-09" class="headerlink" title="2019-09-09"></a>2019-09-09</h3><ul>
<li>刘群：基于深度学习的自然语言处理：边界在哪里？<ul>
<li>自然语言处理的范式迁移：从规则，到统计，再到深度学习（我认为深度学习其实只是统计的延伸，严格意义上还是属于统计的范畴）<ul>
<li>人不可能将所有的规则都穷举出来，并且也写不出大量太细的规则。</li>
</ul>
</li>
<li>深度学习解决了自然语言处理哪些问题？<ul>
<li>词语形态问题（morphology）<ul>
<li>在中文中，它体现在词的切分上</li>
<li>在英语等大部分其他语言中则主要体现在形态分析上。</li>
<li>在统计机器翻译时代，有一个比较著名的方法叫做 Factored statistical machine translation （基于要素的翻译方法）：将一个词分成很多要素，然后分别翻译每个要素，最后汇总要素得到结果。</li>
<li>这个问题在神经网络框架下基本不成问题，因为现在机器翻译基本上可以不做分词，大部分中文机器翻译系统基本上基于汉子实现。</li>
</ul>
</li>
<li>句法结构问题<ul>
<li>在神经网络机器翻译框架下，神经网络可以很好的捕捉句子结构，无需进行句法分析，系统可以自动获得处理复杂结构句子的翻译的能力。</li>
</ul>
</li>
<li>多语言问题<ul>
<li>如果使用中间语言来作文多语言翻译系统的互通桥梁，需要开发的系统的数量随翻译语言的数量呈线性增长；否则，开发系统的数量随翻译语言的数量呈平方增长。</li>
<li>在基于统计方法的机器翻译时代，普遍采用Pivot方法，即在两个语言的互译中，先将所有语言翻译成英语，再翻译成另一种语言。以这种方法来实现多语言翻译。但是这样会导致错误传播和性能下降。</li>
</ul>
</li>
<li>联合训练问题<ul>
<li>在统计机器翻译时代，各个模块相互独立，导致错误传播的问题很严重。如果采用联合训练会导致模型的复杂度大大增加。</li>
<li>神经网络机器翻译框架下，端到端训练很容易实现。</li>
</ul>
</li>
</ul>
</li>
<li>还有哪些深度学习尚未解决的自然语言处理问题？<ul>
<li>资源短缺问题：商业系统的训练语料基本上都是数以千万计的，这在很多专业领域根本收集不到如此庞大的数据，尤其是一些小众语言。</li>
<li>可解释性和可信任问题：神经网络由于计算过程都是在高纬度空间中完成，缺乏可解释性，加之神经网络偶尔犯的一些错误会直接给人带来不信任感。</li>
<li>可控制性问题：当神经网络出现一些错误时我们几乎无法通过修改神经网络来修正这些错误。</li>
<li>超长文本问题：虽然最近提出的BERT、GPT等模型将单词可处理的句子长度扩展到了几百到上千字之多，但是长度超过千字的篇章翻译还是无法一次性完成，还有很多问题尚待解决。</li>
<li>缺乏常识问题：举个🌰。“Bank fishing is fishing from places where the land meets the water’s edge.” 谷歌翻译翻译成“银行捕鱼是从陆地与水边相遇的地方捕鱼的。”</li>
</ul>
</li>
<li>基于深度学习的自然语言处理，其边界在哪里？<ul>
<li>数据边界：数不够</li>
<li>语义边界：深度学习只能对字词符号之间的关系进行建模，并不能对所描述的问题语义或者输入的句子语义进行建模。</li>
<li>符号边界：再好的神经网络也只是在符号之间寻找规律，不能准确的定义或者描述任何特定的表达。</li>
<li>因果边界：通过观察数据能得出的结论永远只有关联性而不可能得出因果性。因果性必须有完备的理论和推理支持。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="2019-08"><a href="#2019-08" class="headerlink" title="2019-08"></a>2019-08</h2><h3 id="2019-08-06"><a href="#2019-08-06" class="headerlink" title="2019-08-06"></a>2019-08-06</h3><ul>
<li>Tensorflow<ul>
<li>tf.expand_dims(input, axis)</li>
<li>tf.concat(values, axis)</li>
<li>tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs)</li>
</ul>
</li>
</ul>
<h3 id="2019-08-02"><a href="#2019-08-02" class="headerlink" title="2019-08-02"></a>2019-08-02</h3><blockquote>
<ul>
<li><div title="陡峭的；完全的；垂直的">sheer</div></li>
<li><div title="观点；态度；意见">sentiment</div></li>
<li><div title="粒度">granularity</div></li>
<li><div title=" 不同种类的，多种多样的；混杂的">miscellaneours</div></li>
<li><div title="轶事的；趣闻的">anecdotal</div></li>
<li><div title="气氛；环境">ambience</div></li>
<li><div title="均匀的；平等的">evenly</div>


</li>
</ul>
</blockquote>
<ul>
<li><a href="https://medium.com/dair-ai/adapters-a-compact-and-extensible-transfer-learning-method-for-nlp-6d18c2399f62" target="_blank" rel="noopener">Adapters: A Compact and Extensible Transfer Learning Method for NLP</a><ul>
<li>在 transformer fine-tuning 的时候插入adapter 模块进行微调，原参数不参加训练，只训练adapter和最上层。</li>
</ul>
</li>
</ul>
<h2 id="2019-07"><a href="#2019-07" class="headerlink" title="2019-07"></a>2019-07</h2><h3 id="2019-07-30"><a href="#2019-07-30" class="headerlink" title="2019-07-30"></a>2019-07-30</h3><ul>
<li>哪些参数对于训练BERT模型至关重要（RoBERTa）：<ul>
<li>static masking VS. dynamic masking:<ul>
<li>static masking: 在数据预处理的时候 mask 一次，每次 epoch 中训练实例使用相同的 mask</li>
<li>dynamic masking： 避免让每次 epock 中的训练数据使用相同的 mask，具体方法是：如果一共训练40个epoch，我们就每训练4个 epoch 就重新进行 masking，这样重复10次，意味着每个训练序列都使用相同的 mask 四次。</li>
<li>动态 masking 的效果比静态的好一丢丢。</li>
</ul>
</li>
<li>模型输入格式和下一局预测：<ul>
<li>这里我没有看明白新智元的论文翻译，得去看一下原文。大致意思应该是他们采用没有NSP的完整句子。</li>
</ul>
</li>
<li>扩大 mini-batches, 并提高学习率</li>
</ul>
</li>
</ul>
<h3 id="2019-07-28"><a href="#2019-07-28" class="headerlink" title="2019-07-28"></a>2019-07-28</h3><blockquote>
<ul>
<li><div title="虚构的事/物"> figment </div>

</li>
</ul>
</blockquote>
<ul>
<li>macbook 突然没有声音解决方法： sudo killall coreaudiod 重置音频核心</li>
</ul>
<h3 id="2019-07-24"><a href="#2019-07-24" class="headerlink" title="2019-07-24"></a>2019-07-24</h3><ul>
<li>mac 复制文件地址： <code>option</code>+<code>commend</code>+<code>C</code></li>
<li>markdown文档中加 <code>&amp;nbsp;</code> 可以加空行</li>
</ul>
<h3 id="2019-07-18"><a href="#2019-07-18" class="headerlink" title="2019-07-18"></a>2019-07-18</h3><blockquote>
<ul>
<li><div title="推测；猜想"> conjecture </div>


</li>
</ul>
</blockquote>
<ul>
<li>AutoGraph: 可以将eager execution执行的tensorflow代码以计算图的方式执行，提高执行效率。<ul>
<li>autograph通过<code>contrib.autograph</code>获取</li>
<li>常用方法：<ul>
<li>修饰: <code>@autograph.convert(optional_features=...)</code></li>
<li>autograph.to_code()</li>
<li>autograph.to_graph()</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2019-07-12"><a href="#2019-07-12" class="headerlink" title="2019-07-12"></a>2019-07-12</h3><blockquote>
<ul>
<li><div title="人造的"> synthetic </div></li>
<li><div title="根本地;彻底地;完全地"> redically </div>


</li>
</ul>
</blockquote>
<h3 id="2019-07-07"><a href="#2019-07-07" class="headerlink" title="2019-07-07"></a>2019-07-07</h3><ul>
<li><p>Linux 换源：</p>
<ul>
<li>apt: /etc/apt/sources.list</li>
<li>pip: ~/.pip/pip.conf</li>
<li>conda: ~/.condarc</li>
</ul>
</li>
<li><p>ssh 服务：</p>
<ul>
<li>开启/关闭开机自启：systemctl enable ssh</li>
<li>单次启动/关闭：systemctl start ssh</li>
</ul>
</li>
</ul>
<h3 id="2019-07-05"><a href="#2019-07-05" class="headerlink" title="2019-07-05"></a>2019-07-05</h3><blockquote>
<ul>
<li><div title="忽视；忽略">neglect</div>

</li>
</ul>
</blockquote>
<h3 id="2019-07-04"><a href="#2019-07-04" class="headerlink" title="2019-07-04"></a>2019-07-04</h3><ul>
<li>Batch Normalization<ul>
<li>一般归一化的是激活函数之前的$z^k$，也有researcher觉得应该归一化激活后的$a^k$</li>
</ul>
</li>
</ul>
<ul>
<li>Paper: Batch Normalization<ul>
<li>Fixing the distribution of the layer inputs $x$ as the training progresses to accelerate the training process.</li>
<li>It has been long known (LeCun’s Efficient BackProp) that the network training converges faster if its inputs are whitened, which means linearly transformed to have zero means and unit variance.</li>
<li>A potential problem is that the gradient descent optimization does not take into account the fact that the normalization takes place. Solution: ensure that, for any parameter values, the network always produces activations with the desired distribution.</li>
<li>The Jacobians</li>
</ul>
</li>
</ul>
<h3 id="2019-07-03"><a href="#2019-07-03" class="headerlink" title="2019-07-03"></a>2019-07-03</h3><blockquote>
<ul>
<li><div title="漂白"> whiten </div> </li>
<li><div title="间隔；幕间休息；间距"> interval </div></li>
<li><div title="散布,散置"> intersperse </div>

</li>
</ul>
</blockquote>
<ul>
<li>deeplearning.ai<ul>
<li>bias VS. variance:<ul>
<li>high bias: underfitting</li>
<li>high variance: overfitting</li>
<li>otherwise: good classifier</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2019-07-02"><a href="#2019-07-02" class="headerlink" title="2019-07-02"></a>2019-07-02</h3><blockquote>
<ul>
<li><div title="臭名昭著的；声名狼藉的"> notorious </div></li>
<li><div title="放大；扩大"> amplify </div></li>
<li><div title="补偿；报酬"> compensate </div></li>
<li>convergence VS. divergence</li>
<li><div title="坚固的；大量的；重大的；实质的"> substantial </div>


</li>
</ul>
</blockquote>
<ul>
<li>Paper: Batch Normalization<ul>
<li>Internal covariate shift: the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change.</li>
<li>It allows us to use much higher learning rates and be less careful about initialization.</li>
<li>It also acts as a regularization, in some cases eliminating the need for Dropout. </li>
<li>Stochastic gradient descent (SGD)</li>
<li>Fixed distribution of inputs to a sub-network would have positive consequences for the layers outside the sub-network.</li>
<li>The saturated regime of the nonlinearity. Could be addressed by using Rectified Linear Units, $ReLU(x)=max(x,0)$, careful initialization, small learning rates.</li>
</ul>
</li>
</ul>
<h2 id="2019-06"><a href="#2019-06" class="headerlink" title="2019-06"></a>2019-06</h2><h3 id="2019-06-28"><a href="#2019-06-28" class="headerlink" title="2019-06-28"></a>2019-06-28</h3><ul>
<li>Paper: Advances in Pre-Training Distributed Word Representations:<ul>
<li>standard cbow model<ul>
<li>the objective of it is to maximize the log-likelihood of the probability of the words given their surrounding:<script type="math/tex; mode=display">\sum^{T}_{t=1}\log p(w_t|C_t)</script></li>
<li>scoring function (simply getting average):<script type="math/tex; mode=display">s(w,C)=\frac{1}{|C|}\sum_{w^\prime \in C}u^T_{w^\prime}v_w</script></li>
<li>word subsampling:<script type="math/tex; mode=display">p_{disc}(w) = 1 - \sqrt{\frac{t}{f_w}}</script></li>
</ul>
</li>
<li>position-dependent weighting<ul>
<li>reweight the context vectors</li>
<li>each position $p$ in a context window is associated with a vector $d_p$, which is learnt from training phase.<script type="math/tex; mode=display">v_C=\sum_{p\in P}d_p</script></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2019-06-27"><a href="#2019-06-27" class="headerlink" title="2019-06-27"></a>2019-06-27</h3><blockquote>
<ul>
<li><div title="未察觉...；无视...">be oblivious to</div></li>
<li><div title="混乱；使凌乱">clutter</div></li>
<li><div title="切线；切线的；离题的">tangent</div></li>
<li><div title="...的关键；难题">the crux of</div>



</li>
</ul>
</blockquote>
<h3 id="2019-06-24"><a href="#2019-06-24" class="headerlink" title="2019-06-24"></a>2019-06-24</h3><ul>
<li>hexo next 主题的图片边框： themes/next/source/css/_common/components/post/post-expand.styl。 部署前不要忘记hexo clean。</li>
<li>word frequency distribution, Zipf distribution, which implies that most of the words belongs to small subset of the entire vovabulary <a href="https://ieeexplore.ieee.org/abstract/document/165464" target="_blank" rel="noopener">(Li, 1992)</a>.</li>
</ul>
<h3 id="2019-06-18"><a href="#2019-06-18" class="headerlink" title="2019-06-18"></a>2019-06-18</h3><blockquote>
<ul>
<li><div title="复数；复数的"> plural </div></li>
<li><div title="杂货店"> grocery </div></li>
<li><div title="选民；构成部分"> constituent </div></li>
<li><div title="微妙的；细微的"> subtle </div></li>
<li><div title="离散的"> diffuse </div></li>
<li><div title="减轻；救济；安慰"> relief </div></li>
<li><div title="激烈的；猛烈的"> drastic </div>

</li>
</ul>
</blockquote>
<ul>
<li><a href="https://colah.github.io/posts/2015-09-Visual-Information/" target="_blank" rel="noopener">Information Theory</a><ul>
<li>entropy = optimal average length in coding problem</li>
<li>prefix codes - prefix property: no codeword should be the prefix of another codeword</li>
<li>short codewords reduce the average message length but are expensive, while long codewords increase the average message length but are cheep. <img src="/2019/02/13/everyday-note/code-cost-longshort.png"></li>
<li><img src="/2019/02/13/everyday-note/calculating-entropy.JPG"></li>
<li>The more concentrated the probability, the more people can craft a clever code with short average messages. The more diffuse the probability, the longer people’s messages have to be.<ul>
<li>If there’s two things that could happen with 50% probability, people only need to send 1 bit. </li>
<li>If there’s 64 different things that could happen with equal probability, they would have to send 6 bits. </li>
</ul>
</li>
<li>entropy VS. cross entropy:<ul>
<li>cross entropy - the average length of communicating an event from one distribution with the optimal code for another distribution: <img src="/2019/02/13/everyday-note/cross-entropy.JPG"></li>
<li>cross-entropy isn’t symmetric. The more different the distributions $p$ and $q$ are, the more the cross-entropy of $p/q$ with respect to $q/p$ will be bigger than the entropy of $p/q$: <img src="/2019/02/13/everyday-note/cross-entropy-unsymmetrical.JPG"></li>
<li>This kind of difference is called Kullback-Leibler (KL) divergence: <script type="math/tex; mode=display">D_q(p)=H_q(p) - H(p)</script>KL divergence like a distance between two distributions. It measures how different they are</li>
</ul>
</li>
<li>conditional entropy: <script type="math/tex; mode=display">\begin{split} H(X|Y) &= \sum_{y}p(y)\sum_{x}p(x|y)log_2(\frac{1}{p(x|y)})\\ &=\sum_{x,y}p(x|y)log_2(\frac{1}{p(x|y)})\end{split}</script></li>
</ul>
</li>
</ul>
<h3 id="2019-06-13"><a href="#2019-06-13" class="headerlink" title="2019-06-13"></a>2019-06-13</h3><blockquote>
<ul>
<li>cumbersome</li>
<li>polysemy</li>
<li>as it were</li>
</ul>
</blockquote>
<ul>
<li>two properties of sigmoid function:<script type="math/tex; mode=display">\sigma(-u) = 1-\sigma(u)</script><script type="math/tex; mode=display">\frac{d\sigma(u)}{du} = \sigma(u)\sigma(-u)</script></li>
</ul>
<h3 id="2019-06-11"><a href="#2019-06-11" class="headerlink" title="2019-06-11"></a>2019-06-11</h3><blockquote>
<ul>
<li>derivation</li>
<li>hierarchical</li>
<li>posterior</li>
</ul>
</blockquote>
<ul>
<li>python 中的 StopIteration 异常。</li>
</ul>
<h3 id="2019-06-10"><a href="#2019-06-10" class="headerlink" title="2019-06-10"></a>2019-06-10</h3><blockquote>
<ul>
<li>abound</li>
</ul>
</blockquote>
<ul>
<li>NLP tag:</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>Scheme</th>
<th style="text-align:center">Begin</th>
<th style="text-align:center">Inside</th>
<th style="text-align:center">End</th>
<th style="text-align:center">Single</th>
<th style="text-align:center">Other</th>
</tr>
</thead>
<tbody>
<tr>
<td>IOB</td>
<td style="text-align:center">B-X</td>
<td style="text-align:center">I-X</td>
<td style="text-align:center">I-X</td>
<td style="text-align:center">B-X</td>
<td style="text-align:center">O</td>
</tr>
<tr>
<td>IOE</td>
<td style="text-align:center">I-X</td>
<td style="text-align:center">I-X</td>
<td style="text-align:center">E-X</td>
<td style="text-align:center">E-X</td>
<td style="text-align:center">O</td>
</tr>
<tr>
<td>IOBES</td>
<td style="text-align:center">B-X</td>
<td style="text-align:center">I-X</td>
<td style="text-align:center">E-X</td>
<td style="text-align:center">S-X</td>
<td style="text-align:center">O</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>Activation Funciton:<ul>
<li>can add non-linearity to the output, so it makes neural network solve non-linear problems</li>
<li>type:<ul>
<li>linear</li>
<li>sigmoid</li>
<li>tanh</li>
<li>Rectified Linear Unit (ReLU)</li>
<li>softmax</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2019-06-04"><a href="#2019-06-04" class="headerlink" title="2019-06-04"></a>2019-06-04</h3><blockquote>
<ul>
<li>derivatives</li>
<li>integrals</li>
<li>differential equations</li>
<li>pendulum</li>
</ul>
</blockquote>
<ul>
<li>3Blue1Brown: 微分方程概论-第一章<ul>
<li>Ordinary Differential Equation(ODE) VS. Partial Diffential Equation(PDE)<ul>
<li>PDE: think of them as involving a continum of values changing with time, like the temperature at every point of a solid body</li>
</ul>
</li>
<li>Second Order Differential Equation:<ul>
<li>whih means the highest derivative you find in this expression is a second derivative</li>
<li>higher-order differential equations include derivatives more than two</li>
</ul>
</li>
<li>phase space</li>
</ul>
</li>
</ul>
<h2 id="2019-05"><a href="#2019-05" class="headerlink" title="2019-05"></a>2019-05</h2><h3 id="2019-05-31"><a href="#2019-05-31" class="headerlink" title="2019-05-31"></a>2019-05-31</h3><ul>
<li>修改graph结构：<code>tf.import_graph_def(graph_def,name=&#39;&#39;, input_map={&#39;model_intent/embedding_lookup:0&#39;: char_embed})</code></li>
</ul>
<h3 id="2019-05-29"><a href="#2019-05-29" class="headerlink" title="2019-05-29"></a>2019-05-29</h3><ul>
<li>python 中 write 和 writeline 区别：write只能以字符串为参数，writeline可以传入字符列表。</li>
</ul>
<h3 id="2019-05-29-1"><a href="#2019-05-29-1" class="headerlink" title="2019-05-29"></a>2019-05-29</h3><blockquote>
<ul>
<li>enact</li>
<li>curb</li>
<li>proliferate</li>
</ul>
</blockquote>
<ul>
<li>最小二乘法：将需要预估的参数作为变量，对每个变量求偏导，求每个变量倒数为0的解: <script type="math/tex; mode=display">\hat{\beta}=(X^TX)^{-1}X^Ty</script></li>
</ul>
<h3 id="2019-05-21"><a href="#2019-05-21" class="headerlink" title="2019-05-21"></a>2019-05-21</h3><ul>
<li>在序列标准任务中，逐帧softmax和条件随机场（CRF）的根本不同：<strong>前者将序列标注看成是n个k分类问题，后者将序列标注看成是1个 $k^n$ 分类问题</strong>。</li>
</ul>
<h3 id="2019-05-15"><a href="#2019-05-15" class="headerlink" title="2019-05-15"></a>2019-05-15</h3><blockquote>
<p>生词：</p>
<ul>
<li>imaginary friend</li>
</ul>
</blockquote>
<ul>
<li>Information Theory:<ul>
<li>Simpson’s Paradox  <img src="/2019/02/13/everyday-note/simpson-separated-note.png"></li>
<li>code:<ul>
<li>prefix property: no codeword should be the prefix of another codeword</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2019-05-14"><a href="#2019-05-14" class="headerlink" title="2019-05-14"></a>2019-05-14</h3><blockquote>
<p>生词：</p>
<ul>
<li>toll</li>
<li>optic</li>
<li>retina</li>
<li>stave off</li>
<li>gadget</li>
<li>unerringly</li>
<li>I have <strong>misgiving</strong> about…</li>
<li>intimidate</li>
</ul>
</blockquote>
<h3 id="2019-05-09"><a href="#2019-05-09" class="headerlink" title="2019-05-09"></a>2019-05-09</h3><blockquote>
<p>生词：</p>
<ul>
<li>metabolism</li>
<li>keeps on ticking</li>
<li>glucose</li>
<li>rev up</li>
<li>molecular</li>
<li>carbs</li>
<li>exercise regimen</li>
<li>stunning feat of plasticity</li>
</ul>
</blockquote>
<ul>
<li>词袋模型 bag-of-word</li>
</ul>
<h3 id="2019-05-05"><a href="#2019-05-05" class="headerlink" title="2019-05-05"></a>2019-05-05</h3><ul>
<li>Transformer:<ul>
<li>encoder 最后一层的输出会在decoder每一层被用到。</li>
<li>multi-head attention 层是为了找出tokens之间的关系，之后的add是为了让它不要忘记自己。</li>
<li>the Dot-Product Attention for head $i$ : <script type="math/tex; mode=display">Attention(Q_i,K_i,V_i) = softmax(\frac{Q_iK_i^{T}}{\sqrt[]{d_k}})V_i</script><ul>
<li>其中：<ul>
<li>$Q_i = XW_i^Q$ ，维度为 $input\_length \times d_k$</li>
<li>$K_i = XW_i^K$ ，维度为 $input\_length \times d_k$</li>
<li>$V_i = XW_i^V$ ，维度为 $input\_length \times d_v$</li>
</ul>
</li>
<li>论文中（<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention is all your need</a>）使用$d_k=d_v=\frac{emb\_dim}{h}$</li>
<li>不同 <em>head</em> 不共享$W_i^Q$，$W_i^K$和$W_i^V$，且都是随机初始化。</li>
<li>decoder block 里有两层multi-head attention加一层feed forward，第一层attention用来找出target sequence里的关系（需要用到mask，只允许看对应token的左边的token），第二层attention结合encoder的输出（E）和decoder上一个block的输出（D）,是为了用encoder的input sequence中各个token来表示target sequence中的各个token,相应的：<ul>
<li>$Q_i = DW_i^Q$ ，维度为 $target\_length \times d_k$</li>
<li>$K_i = EW_i^K$ ，维度为 $input\_length \times d_k$</li>
<li>$V_i = EW_i^K$ ，维度为 $input\_length \times d_v$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="2019-04"><a href="#2019-04" class="headerlink" title="2019-04"></a>2019-04</h2><h3 id="2019-04-25"><a href="#2019-04-25" class="headerlink" title="2019-04-25"></a>2019-04-25</h3><blockquote>
<p>生词：</p>
<ul>
<li><strong>diverse</strong> application domains</li>
<li>propose a <strong>taxonomy</strong></li>
</ul>
</blockquote>
<h3 id="2019-04-19"><a href="#2019-04-19" class="headerlink" title="2019-04-19"></a>2019-04-19</h3><blockquote>
<p>生词：</p>
<ul>
<li><strong>granular</strong> cell</li>
<li><strong>scrutinize</strong> over the concept</li>
</ul>
</blockquote>
<ul>
<li>word2vector<ul>
<li>高频词汇采样</li>
<li>负采样: 每次只更新target词和随机几个（5-20）“negative word”对应的权重，减小计算量，加快训练过程。</li>
</ul>
</li>
</ul>
<h3 id="2019-04-17"><a href="#2019-04-17" class="headerlink" title="2019-04-17"></a>2019-04-17</h3><blockquote>
<p>生词：</p>
<ul>
<li>chronic</li>
<li>exceedingly</li>
<li>bound to be exceedingly limited</li>
</ul>
</blockquote>
<ul>
<li>三星手机添加notification提示音：<ul>
<li>在手机文件个目录里新建一个‘Media’文件夹</li>
<li>在‘Media’文件夹里新建‘Notifications’文件夹</li>
<li>在‘Notifications’里添加<code>.MP3</code>文件</li>
</ul>
</li>
</ul>
<h3 id="2019-04-15"><a href="#2019-04-15" class="headerlink" title="2019-04-15"></a>2019-04-15</h3><ul>
<li>Discriminative Model VS. Generative Model<ul>
<li>Discriminative: they model the decision boundary between the different classes, such as Logistic Regression which based on Maximum Likelihood.</li>
<li>Generative model: they model how the data was generated can be used to make classifications, such as Naive Bayes.</li>
</ul>
</li>
<li><a href="https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21?source=email-b01757374e14-1555187113798-digest.reader------1-58------------------811dbe59_37a4_4c19_9db6_ee6b790471a8-1&amp;sectionName=top" target="_blank" rel="noopener">Debugging Neural Networks</a><ol>
<li>Start simple:<ul>
<li>Building a simpler model first. Then gradually add model complexity.</li>
<li>Train your model on a single data point. Using 1 or 2 training data points to confirm whether model is able to overfit, which means it immediately overfit with a training accuracy of 100% and random guessing in validation phase.</li>
</ul>
</li>
<li>Confirm your loss:<ul>
<li>The loss is appropriate for the task(choosing cross-entropy loss or mean squared error?)</li>
<li>Your loss functions are being measured on the correct scale. It’s best to first check the data loss alone (so set regularization strength to zero).</li>
</ul>
</li>
<li>Check intermediate outputs and connections</li>
<li>Diagnose parameters<ul>
<li>Batch size: <ul>
<li>Small batch sizes will result in a learning process that converges quickly at the cost of noise in the training process and might lead to optimization difficulties.</li>
<li>Large-batch methods tend to converge to sharp minimizers of the training and testing functions-and as is well known, sharp minima lead to poorer generalization.</li>
</ul>
</li>
<li>Learning rate: Consider incorporating learning rate scheduling to decrease the learning rate as training progresses</li>
<li>Gradient clipping: Useful for addressing any exploding gradients.</li>
<li>Batch normalization: Fight the internal covariate shift problem.</li>
<li>Stochastic Gradient Descent (SGD): A recommended starting point is Adam or plain SGD with Nesterov momentum.</li>
<li>Regularization: </li>
<li>Dropout</li>
</ul>
</li>
<li>Track your work</li>
</ol>
</li>
</ul>
<h3 id="2019-04-11"><a href="#2019-04-11" class="headerlink" title="2019-04-11"></a>2019-04-11</h3><blockquote>
<p>生词：</p>
<ul>
<li>get accustomed to</li>
<li>take on</li>
<li>leverage</li>
<li>prominent</li>
<li>autonomy</li>
</ul>
</blockquote>
<h3 id="2019-04-09"><a href="#2019-04-09" class="headerlink" title="2019-04-09"></a>2019-04-09</h3><blockquote>
<p>生词：</p>
<ul>
<li>unary</li>
<li>sophisticated</li>
<li>prevalent</li>
<li>latent</li>
</ul>
</blockquote>
<ul>
<li>Transfer Learning:<ul>
<li>transfer learning is the process of trainibg a model on a large-scale dataset and then using that pretrained model to conduct learning for another downstream task.</li>
<li>recent work:<ul>
<li><a href="https://arxiv.org/abs/1801.06146" target="_blank" rel="noopener">ULMFit</a></li>
<li><a href="https://allennlp.org/elmo" target="_blank" rel="noopener">ELMo</a>: 2 layers of LSTM</li>
<li><a href="https://arxiv.org/abs/1806.05662" target="_blank" rel="noopener">GLoMo</a></li>
<li><a href="https://blog.openai.com/language-unsupervised/" target="_blank" rel="noopener">OpenAI transformer</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2019-04-04"><a href="#2019-04-04" class="headerlink" title="2019-04-04"></a>2019-04-04</h3><blockquote>
<p>生词：</p>
<ul>
<li>cope with</li>
<li>be composed of</li>
<li>sanitary</li>
<li>sock</li>
<li>disinfectant</li>
<li>armrest</li>
<li>fungal</li>
</ul>
</blockquote>
<h3 id="2019-04-03"><a href="#2019-04-03" class="headerlink" title="2019-04-03"></a>2019-04-03</h3><blockquote>
<p>生词：</p>
<ul>
<li>have a <strong>self-consciousness</strong></li>
<li>bungling goon</li>
<li>play is <strong>innate</strong></li>
<li>for the sake of</li>
<li><strong>befuddled</strong> aunt/uncle/parent</li>
<li><strong>enamored</strong> with face</li>
<li>recess</li>
<li>paramount</li>
<li>wane</li>
<li>sophisticate</li>
</ul>
</blockquote>
<ul>
<li>How to interact with children:<ul>
<li>Ages 0-1:<ul>
<li>attraction to faces (expressions) only grows from there</li>
<li>speak as much as possible using baby talk with infant</li>
</ul>
</li>
<li>Ages 1-3:<ul>
<li>wait for an invitation into the child’s world. Follow their verbal and nonverbal cues.</li>
</ul>
</li>
<li>Ages 3-5:<ul>
<li>don’t push child to learning school readiness and other skills.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2019-04-02"><a href="#2019-04-02" class="headerlink" title="2019-04-02"></a>2019-04-02</h3><blockquote>
<p>生词：</p>
<ul>
<li>obsessive-compulsive disorder(OCD)</li>
</ul>
</blockquote>
<h2 id="2019-03"><a href="#2019-03" class="headerlink" title="2019-03"></a>2019-03</h2><h3 id="2019-03-30"><a href="#2019-03-30" class="headerlink" title="2019-03-30"></a>2019-03-30</h3><!-- more -->
<blockquote>
<p>生词:</p>
<ul>
<li>dodge</li>
<li>legitimate</li>
<li>folk</li>
<li>profitable</li>
</ul>
</blockquote>
<h3 id="2019-03-28"><a href="#2019-03-28" class="headerlink" title="2019-03-28"></a>2019-03-28</h3><blockquote>
<p>生词：</p>
<ul>
<li>grasp</li>
<li>slang</li>
</ul>
</blockquote>
<ul>
<li><a href="https://www.tensorflow.org/tutorials/representation/word2vec" target="_blank" rel="noopener">Word2vec</a>：<ul>
<li>有两种类型<ul>
<li>连续词袋模型（CBOW）：根据上下文预测目标字词</li>
<li>Skip-Gram：根据从目标字词中预测上下文字词</li>
</ul>
</li>
<li>噪声对比估算（NCE）损失是基于逻辑回归模型进行定义的</li>
</ul>
</li>
<li><a href="https://nlp.stanford.edu/projects/glove/" target="_blank" rel="noopener">GloVe</a>:<ul>
<li>先从文本中处理得到一个word-word co-occurrence矩阵 <img src="/2019/02/13/everyday-note/word-word-co-occurrence-matrix.png" title="word-word co-occurrence矩阵的一个小栗子"></li>
<li>训练过程是让词向量的点乘结果尽可能等于相应两个词一起出现的概率的对数。</li>
</ul>
</li>
</ul>
<h3 id="2019-03-26"><a href="#2019-03-26" class="headerlink" title="2019-03-26"></a>2019-03-26</h3><blockquote>
<p>生词：</p>
<ul>
<li>meteor</li>
<li>splash</li>
<li>bruise</li>
<li>shatter</li>
<li>flock </li>
<li>patio</li>
<li>bureaucratic</li>
<li>unlikely</li>
<li>trickle</li>
<li>deficiency</li>
<li>patriotic</li>
</ul>
</blockquote>
<ul>
<li>vim:<ul>
<li>v: 可视模式</li>
<li>w/b：前一个单词/后一个单词</li>
<li>^：本行第一个不是blank字符的位置</li>
<li>0：本行第一个位置</li>
<li>$：本行行尾</li>
<li>g_：本行最后一个不是blank字符的位置</li>
<li>p：黏贴</li>
<li>u：undo</li>
<li>CTRL-r：redo</li>
<li>d: 删除并复制</li>
<li>y：直接复制</li>
</ul>
</li>
</ul>
<h3 id="2019-03-21"><a href="#2019-03-21" class="headerlink" title="2019-03-21"></a>2019-03-21</h3><ul>
<li>从远程Git库中下在分支到本地：<ul>
<li><code>git pull origin branch_name:local_name</code> ：获取远程库中的branch_name分支与本地的local_name分支进行merge，如果要与本地当前分支进行merge可以不写冒号和冒号后面内容，如果本地所在的当前分支（dev）与对应远程分支（origin/dev）已建立联系，只需 <code>git pull origin</code></li>
<li><code>git checkout -b local_name origin/branch_name</code>: 自动创建一个新的本地分支（local_name），并与指定远程分支关联起来。</li>
</ul>
</li>
</ul>
<h3 id="2019-03-19"><a href="#2019-03-19" class="headerlink" title="2019-03-19"></a>2019-03-19</h3><ul>
<li>支持向量机 (Support Vector Machine)：<ul>
<li>支持向量 (support vectors):<ul>
<li>support vectores are the samples that are most difficult to classify</li>
<li>they directly affect the process to find the optimum location of the decision boundaries.</li>
</ul>
</li>
<li>constrained optimization - Lagrange Multipliers<ul>
<li>obtaining extrema of function $f(x,y)$ under the constraint $g(x,y) = A$ -&gt; $f^\prime(x,y) = \lambda g^\prime(x,y)$:</li>
</ul>
</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">\begin{split}
f^\prime (x,y) &= [f(x,y) + \lambda(A-g(x,y))]^\prime\\
 &=f^\prime(x,y) - \lambda g^\prime(x,y) = 0
\end{split} \tag{19.03.19.1}</script><ul>
<li>Math in Markdown<ul>
<li>one expression in multi-lines : \begin{split} align by ‘&amp;’ before ‘=’</li>
<li>multi-expressions in multi-lines : \begin{eqnarray*} align by ‘&amp;…&amp;’ wrap ‘=’</li>
</ul>
</li>
</ul>
<h3 id="2019-03-06"><a href="#2019-03-06" class="headerlink" title="2019-03-06"></a>2019-03-06</h3><blockquote>
<p>生词：</p>
<ul>
<li>treadmill</li>
<li>stationary</li>
<li>rack up</li>
<li>bold</li>
<li>obligation</li>
</ul>
</blockquote>
<h3 id="2019-03-05"><a href="#2019-03-05" class="headerlink" title="2019-03-05"></a>2019-03-05</h3><ul>
<li>Conda 操作：<ul>
<li>信息： <code>conda info</code></li>
<li>更新自己： <code>conda update conda</code></li>
<li>安装/更新 包：<code>conda install/update [package]</code></li>
<li>创建一个新python环境：<code>conda create --name mingzi python=3.6</code></li>
<li>激活一个python环境：<code>activate mingzi</code> (Win)/<code>source activate mingzi</code> (Linux/mac)</li>
<li>取消激活当前环境：<code>deactivate</code> (Win)/ <code>source deactivate</code> (Linux/mac)</li>
<li>环境列表：<code>conda env list</code></li>
</ul>
</li>
</ul>
<h3 id="2019-03-04"><a href="#2019-03-04" class="headerlink" title="2019-03-04"></a>2019-03-04</h3><ul>
<li>Vim 操作：<ul>
<li>dd -&gt; 删除当前行，并将这行保存到剪切板里</li>
<li>p -&gt; 粘贴剪切板里的内容</li>
<li>h，j，k，l -&gt; 移动光标（方向键也可以）</li>
<li>：help \<command> -&gt;  显示相关命令的帮助</li>
<li>u -&gt; 撤销（undo）</li>
</ul>
</li>
</ul>
<h2 id="2019-02"><a href="#2019-02" class="headerlink" title="2019-02"></a>2019-02</h2><h3 id="2019-02-28"><a href="#2019-02-28" class="headerlink" title="2019-02-28"></a>2019-02-28</h3><blockquote>
<p>生词：</p>
<ul>
<li>arithmetic</li>
</ul>
</blockquote>
<ul>
<li>Attention mechanism:<ul>
<li>attention 计算时(softmax之前)，每个待加权部分（$y_i$）的权重只跟上下文（context）$C$ 有关，与别的部分（$y_j, j \neq i$）无关。如下图： <img src="/2019/02/13/everyday-note/attention.JPG"></li>
<li>Soft Attention vs. Hard Attention<ul>
<li>soft attention: 结果为各个待加权部分的加权和</li>
<li>hard attention：结果为根据相应概率随机选取的一个待加权部分</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2019-02-19"><a href="#2019-02-19" class="headerlink" title="2019-02-19"></a>2019-02-19</h3><blockquote>
<p>生词：</p>
<ul>
<li>tickle</li>
<li>sibling</li>
<li>stroke a cat</li>
<li>discrepancy</li>
<li>procrastinate</li>
<li>prone</li>
</ul>
</blockquote>
<ul>
<li>scp命令：<code>scp test.txt  account@ip:/home/xxx/test.txt</code> </li>
</ul>
<h3 id="2019-02-17"><a href="#2019-02-17" class="headerlink" title="2019-02-17"></a>2019-02-17</h3><blockquote>
<p>生词：</p>
<ul>
<li>anecdote</li>
<li>instill</li>
<li>ostracism/banishment</li>
<li>squeak</li>
</ul>
</blockquote>
<ul>
<li>向量叉乘（cross product）， 如果逆时针方向相乘则结果为正，反之结果为负。叉乘的结果（三维空间）为一个向量，这个向量的大小是两个相乘的向量所构成的平行四边形的面积，方向与该平行四边形垂直。求解两个向量的叉乘可以用求解对应行列式的方法（加上一个向量 ($\hat{i},\hat{j}, \hat{k}$)）</li>
<li>线性的严格定义（Formal definition of linearity）:<ul>
<li>可加性： $L(\vec{v}+\vec{w})=L(\vec{v})+L(\vec{w})$</li>
<li>成比性：$L(c\vec{v})=cL(\vec{v})$</li>
</ul>
</li>
<li>因此，函数只要满足上面的规定也是线性的。</li>
<li>The learning your do learning forward can be substantially more efficient if you have all the right intuitions in places.</li>
</ul>
<h3 id="2019-02-15"><a href="#2019-02-15" class="headerlink" title="2019-02-15"></a>2019-02-15</h3><blockquote>
<p>生词：</p>
<ul>
<li>perusal</li>
<li>connotation</li>
<li>fumble</li>
<li>clumsily</li>
<li>intercept</li>
<li>distort</li>
<li>perception</li>
<li>clutter</li>
<li>duality</li>
</ul>
</blockquote>
<ul>
<li><code>GraphDef</code> 是TensorFlow用来保存/加载<code>Graph</code>时的中间对象。保存时，可以通过<code>Graph.as_graph_def()</code>或者<code>Session.graph_def</code>获取到<code>GraphDef</code>对象用于序列化保存到文件；加载时，也需要先用<code>tf.GraphDef()</code>获取一个空<code>GraphDef</code>对象然后用 <strong>.pb</strong> 文件中的数据来填充（<code>graph_def.ParseFromString(f.read())</code>）,最后通过<code>tf.import_graph_def(graph_def, name=&#39;prefix&#39;)</code>的形式加载到指定的Graph中。</li>
<li>对偶性： 两种数学事物之间自然而又出乎意料的对应关系</li>
</ul>
<h3 id="2019-02-14"><a href="#2019-02-14" class="headerlink" title="2019-02-14"></a>2019-02-14</h3><blockquote>
<p>生词：</p>
<ul>
<li>spouse</li>
<li>tepastry</li>
<li>gargantuan</li>
</ul>
</blockquote>
<ul>
<li><p>用 <code>tf.train.Saver()</code> 保存/载入模型：</p>
<ul>
<li><p><strong>.data</strong> 文件保存权重（weights）；<strong>.meta</strong> 文件保存计算图的各种元数据（metadata）比如学习率和优化器； <strong>.index</strong> 文件保存键-值对，用来根据模型中的tensor name在 .data 文件中找到相应的data。</p>
<ul>
<li><strong>载入图结构和元数据</strong> 用 <code>tf.train.import_meta_graph(&#39;models/model.ckpt-1000.meta&#39;)</code>, 该方法自动将 “models/model.ckpt-1000.meta” 文件中保存的图加载到默认图（default_graph）中并返回一个 <code>Saver</code>。</li>
<li><strong>回复/载入权重（weights）</strong> 的过程跟载入图结构和元数据不同的是它必须在要给 <code>Session</code> 中执行，可以把它理解为权重的初始化类似的过程，如下：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment">#用载入权重的方式来初始化权重，就不需要tf.global_variables_initializer()过程了</span></span><br><span class="line">    saver.restore(sess, <span class="string">'models/model.ckpt.data-1000-00000-of-00001'</span>)</span><br><span class="line">    print(sess.run(global_step_tensor))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>可以声明多个 <code>Saver</code> 来保存所有或特定变量（<code>Variables</code>）并用字典({name:Var})的形式给变量指定名字：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">v1 = tf.Variable(<span class="number">1.</span>, name=<span class="string">"v1"</span>)</span><br><span class="line">all_saver = tf.train.Saver()</span><br><span class="line">v1_saver = tf.train.Saver(&#123;<span class="string">"V1"</span>:v1&#125;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>一个 <code>Saver</code> 只能保存当前 <code>Session</code> 对应 <code>Graph</code> 中的变量,所以程序中有多个 <code>Graph</code> 时要注意 <code>Saver</code> 的使用，下面这样使用 <code>Saver</code> 就会报错：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">g = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    v1 = tf.Variable(<span class="number">5.</span>, dtype=tf.float32)</span><br><span class="line">print(v1.graph == g) <span class="comment"># 输出True</span></span><br><span class="line">print(v1.graph == tf.get_default_graph()) <span class="comment"># 输出False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个Saver如果在绑定默认default_graph的Session中执行会报错，因为v1是在 g 中的变量而不是default_graph中</span></span><br><span class="line">saver = tf.train.Saver([v1])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    saver.save(sess, <span class="string">'./test'</span>) <span class="comment">#执行这句代码时会报错</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>用 <code>tf.saved_model.simple_save</code> 保存模型：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">simple_save(session,</span><br><span class="line">            export_dir,</span><br><span class="line">            inputs=&#123;<span class="string">"x"</span>: x, <span class="string">"y"</span>: y&#125;),</span><br><span class="line">            outputs=&#123;<span class="string">"z"</span>: z&#125;)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>.pb</strong> 文件保存图结构和元数据； <strong>variables</strong> 文件夹内保存当前的权重。</li>
<li>加载saved_model：  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export_dir = ...</span><br><span class="line">...</span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=tf.Graph()) <span class="keyword">as</span> sess:</span><br><span class="line">    tf.saved_model.loader.load(sess, [tag_constants.TRAINING], export_dir)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>把模型的所有信息（包括图数据和权重信息）全部保存到一个 .pb 文件中：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.python.framework <span class="keyword">import</span> graph_util</span><br><span class="line">output_graph_def = graph_util.convert_variables_to_constants(session, </span><br><span class="line">                                                             session.graph_def,</span><br><span class="line">                                                             output_node_names=[</span><br><span class="line">                                                                         <span class="string">'model_intent/intent_scores'</span>])</span><br><span class="line"><span class="keyword">with</span> tf.gfile.FastGFile(<span class="string">'models/intent_model.pb'</span>, mode=<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(output_graph_def.SerializeToString())</span><br></pre></td></tr></table></figure>
<p>  加载保存的 .pb 文件：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_graph</span><span class="params">(frozen_graph_filename)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.gfile.GFile(frozen_graph_filename, <span class="string">"rb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        graph_def = tf.GraphDef()              <span class="comment"># 创建一个空的graph_def</span></span><br><span class="line">        graph_def.ParseFromString(f.read())    <span class="comment"># 用 .pb 文件中保存的对象填充该graph_def</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Graph().as_default() <span class="keyword">as</span> graph:</span><br><span class="line">        <span class="comment"># 下面的name参数的值会作为graph中所有op的前缀</span></span><br><span class="line">        tf.import_graph_def(graph_def, name=<span class="string">"prefix"</span>)</span><br><span class="line">    <span class="keyword">return</span> graph</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2019-02-13"><a href="#2019-02-13" class="headerlink" title="2019-02-13"></a>2019-02-13</h3><ul>
<li>线性变换（Linear transformation）就是输入一个向量然后输出另一个向量。这里的transformation(变换)和function（函数）同义，之所以用transformation是因为线性代数里面的线性变换是可以在坐标系中（低维）可视化表现出来的。前面的形容词Linear的意思是：整个坐标系变换之后直线依旧是直线，原点保持固定。下面两个变换都不是线性变换(灰线表示原坐标系)。</li>
</ul>
<img src="/2019/02/13/everyday-note/unlinear.JPG"> <img src="/2019/02/13/everyday-note/unlinear1.JPG">
<ul>
<li>矩阵乘法相当于两个线性变换的复合。类似：<img src="/2019/02/13/everyday-note/Composition.JPG"> 线性变换的行列式 可以理解为线性变换后原单位面积被拉伸/压缩的倍数。 用这个概念很好理解：<script type="math/tex; mode=display">det(M_1M_2)=det(M_1)det(M_2)</script><strong>矩阵的逆</strong> 在transformation上的变现形式就是逆变换（顺时针旋转90度对应的矩阵的逆就是逆时针旋转90度对应的矩阵），因此矩阵A乘以矩阵A的逆表示变换之后又逆变换，相当于什么都没做.</li>
<li>矩阵（或者说线性变换）的 秩（rank）可以理解为线性变换结束后整个坐标系的维度（维度是否被压缩，或者保持原维度）。更加严谨的定义是矩阵列空间的维度（换句话说就是所有列向量可以在几维空间里表示）。所以顾名思义，满秩，表示任何两个列向量都不线性相关即秩数等于列向量的个数即达到最大值。</li>
<li>零空间（Null space），一些变换后落在零向量上的向量构成的空间。</li>
</ul>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/07/08/MachineLearning-ZZH/" rel="next" title="读书笔记：《机器学习》by周志华 (阅读中...)">
                <i class="fa fa-chevron-left"></i> 读书笔记：《机器学习》by周志华 (阅读中...)
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/06/23/visual-information-theory/" rel="prev" title="看图理解信息理论中的“熵”、“交叉熵”等概念">
                看图理解信息理论中的“熵”、“交叉熵”等概念 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/uploads/xis.jpg" alt="欢乐一只虾">
            
              <p class="site-author-name" itemprop="name">欢乐一只虾</p>
              <div class="site-description motion-element" itemprop="description">xia写的</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">9</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">5</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">23</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#2020"><span class="nav-text">2020</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2020-02"><span class="nav-text">2020-02</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2020-02-26"><span class="nav-text">2020-02-26</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2020-02-25"><span class="nav-text">2020-02-25</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2020-02-23"><span class="nav-text">2020-02-23</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2020-02-22"><span class="nav-text">2020-02-22</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2020-02-21"><span class="nav-text">2020-02-21</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2020-02-20"><span class="nav-text">2020-02-20</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2020-02-15"><span class="nav-text">2020-02-15</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2020-02-11"><span class="nav-text">2020-02-11</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2020-02-10"><span class="nav-text">2020-02-10</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2020-02-07"><span class="nav-text">2020-02-07</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2020-02-06"><span class="nav-text">2020-02-06</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2020-02-04"><span class="nav-text">2020-02-04</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2020-01"><span class="nav-text">2020-01</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2020-01-21"><span class="nav-text">2020-01-21</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2020-01-19"><span class="nav-text">2020-01-19</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2020-01-15"><span class="nav-text">2020-01-15</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2020-01-13"><span class="nav-text">2020-01-13</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2020-01-09"><span class="nav-text">2020-01-09</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2020-01-08"><span class="nav-text">2020-01-08</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2020-01-07"><span class="nav-text">2020-01-07</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2020-01-06"><span class="nav-text">2020-01-06</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2020-01-03"><span class="nav-text">2020-01-03</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2019"><span class="nav-text">2019</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2019-12"><span class="nav-text">2019-12</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-12-19"><span class="nav-text">2019-12-19</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-12-18"><span class="nav-text">2019-12-18</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019-11"><span class="nav-text">2019-11</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-11-25"><span class="nav-text">2019-11-25</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-11-21"><span class="nav-text">2019-11-21</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-11-20"><span class="nav-text">2019-11-20</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-11-19"><span class="nav-text">2019-11-19</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-11-13"><span class="nav-text">2019-11-13</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-11-12"><span class="nav-text">2019-11-12</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-11-11"><span class="nav-text">2019-11-11</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019-10"><span class="nav-text">2019-10</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-10-31"><span class="nav-text">2019-10-31</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-10-27"><span class="nav-text">2019-10-27</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-10-17"><span class="nav-text">2019-10-17</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-10-10"><span class="nav-text">2019-10-10</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019-09"><span class="nav-text">2019-09</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-09-29"><span class="nav-text">2019-09-29</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-09-27"><span class="nav-text">2019-09-27</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-09-26"><span class="nav-text">2019-09-26</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-09-25"><span class="nav-text">2019-09-25</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-09-24"><span class="nav-text">2019-09-24</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-09-23"><span class="nav-text">2019-09-23</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-09-10"><span class="nav-text">2019-09-10</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-09-09"><span class="nav-text">2019-09-09</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019-08"><span class="nav-text">2019-08</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-08-06"><span class="nav-text">2019-08-06</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-08-02"><span class="nav-text">2019-08-02</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019-07"><span class="nav-text">2019-07</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-07-30"><span class="nav-text">2019-07-30</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-07-28"><span class="nav-text">2019-07-28</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-07-24"><span class="nav-text">2019-07-24</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-07-18"><span class="nav-text">2019-07-18</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-07-12"><span class="nav-text">2019-07-12</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-07-07"><span class="nav-text">2019-07-07</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-07-05"><span class="nav-text">2019-07-05</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-07-04"><span class="nav-text">2019-07-04</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-07-03"><span class="nav-text">2019-07-03</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-07-02"><span class="nav-text">2019-07-02</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019-06"><span class="nav-text">2019-06</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-06-28"><span class="nav-text">2019-06-28</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-06-27"><span class="nav-text">2019-06-27</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-06-24"><span class="nav-text">2019-06-24</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-06-18"><span class="nav-text">2019-06-18</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-06-13"><span class="nav-text">2019-06-13</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-06-11"><span class="nav-text">2019-06-11</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-06-10"><span class="nav-text">2019-06-10</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-06-04"><span class="nav-text">2019-06-04</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019-05"><span class="nav-text">2019-05</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-05-31"><span class="nav-text">2019-05-31</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-05-29"><span class="nav-text">2019-05-29</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-05-29-1"><span class="nav-text">2019-05-29</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-05-21"><span class="nav-text">2019-05-21</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-05-15"><span class="nav-text">2019-05-15</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-05-14"><span class="nav-text">2019-05-14</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-05-09"><span class="nav-text">2019-05-09</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-05-05"><span class="nav-text">2019-05-05</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019-04"><span class="nav-text">2019-04</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-04-25"><span class="nav-text">2019-04-25</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-04-19"><span class="nav-text">2019-04-19</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-04-17"><span class="nav-text">2019-04-17</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-04-15"><span class="nav-text">2019-04-15</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-04-11"><span class="nav-text">2019-04-11</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-04-09"><span class="nav-text">2019-04-09</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-04-04"><span class="nav-text">2019-04-04</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-04-03"><span class="nav-text">2019-04-03</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-04-02"><span class="nav-text">2019-04-02</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019-03"><span class="nav-text">2019-03</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-03-30"><span class="nav-text">2019-03-30</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-03-28"><span class="nav-text">2019-03-28</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-03-26"><span class="nav-text">2019-03-26</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-03-21"><span class="nav-text">2019-03-21</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-03-19"><span class="nav-text">2019-03-19</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-03-06"><span class="nav-text">2019-03-06</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-03-05"><span class="nav-text">2019-03-05</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-03-04"><span class="nav-text">2019-03-04</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019-02"><span class="nav-text">2019-02</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-02-28"><span class="nav-text">2019-02-28</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-02-19"><span class="nav-text">2019-02-19</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-02-17"><span class="nav-text">2019-02-17</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-02-15"><span class="nav-text">2019-02-15</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-02-14"><span class="nav-text">2019-02-14</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-02-13"><span class="nav-text">2019-02-13</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">欢乐一只虾</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
    <span title="站点总字数">68k</span>
  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a></div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="post-meta-divider">|</span>
  

  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>












  













  



  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script src="/lib/fancybox/source/jquery.fancybox.pack.js"></script>

  
  <script src="/lib/reading_progress/reading_progress.js"></script>


  


  <script src="/js/utils.js?v=7.1.2"></script>

  <script src="/js/motion.js?v=7.1.2"></script>



  
  


  <script src="/js/affix.js?v=7.1.2"></script>

  <script src="/js/schemes/pisces.js?v=7.1.2"></script>




  
  <script src="/js/scrollspy.js?v=7.1.2"></script>
<script src="/js/post-details.js?v=7.1.2"></script>



  


  <script src="/js/next-boot.js?v=7.1.2"></script>


  
  <script src="/js/js.cookie.js?v=7.1.2"></script>
  <script src="/js/scroll-cookie.js?v=7.1.2"></script>


  

  
  
  

  

  
  
  


  


  


  




  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
