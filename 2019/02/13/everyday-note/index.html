<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">






  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">











  
  
  <link rel="stylesheet" href="/lib/fancybox/source/jquery.fancybox.css">




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  

  
    
    
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mina:300,300italic,400,400italic,700,700italic|Lato:300,300italic,400,400italic,700,700italic|Indie Flower:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext">
  






<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">

<link rel="stylesheet" href="/css/main.css?v=7.1.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/uploads/favicon-32x32-xia.png?v=7.1.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/uploads/favicon-16x16-xia.png?v=7.1.2">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.2" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: true,
    fancybox: true,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="每天记点小笔记我也可以拿第一">
<meta property="og:type" content="article">
<meta property="og:title" content="笔记本">
<meta property="og:url" content="http://yoursite.com/2019/02/13/everyday-note/index.html">
<meta property="og:site_name" content="Xia&#39;s Blog">
<meta property="og:description" content="每天记点小笔记我也可以拿第一">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://yoursite.com/2019/02/13/everyday-note/code-cost-longshort.png">
<meta property="og:image" content="http://yoursite.com/2019/02/13/everyday-note/calculating-entropy.JPG">
<meta property="og:image" content="http://yoursite.com/2019/02/13/everyday-note/cross-entropy.JPG">
<meta property="og:image" content="http://yoursite.com/2019/02/13/everyday-note/cross-entropy-unsymmetrical.JPG">
<meta property="og:image" content="http://yoursite.com/2019/02/13/everyday-note/simpson-separated-note.png">
<meta property="og:image" content="http://yoursite.com/2019/02/13/everyday-note/word-word-co-occurrence-matrix.png">
<meta property="og:image" content="http://yoursite.com/2019/02/13/everyday-note/attention.JPG">
<meta property="og:image" content="http://yoursite.com/2019/02/13/everyday-note/unlinear.JPG">
<meta property="og:image" content="http://yoursite.com/2019/02/13/everyday-note/unlinear1.JPG">
<meta property="og:image" content="http://yoursite.com/2019/02/13/everyday-note/Composition.JPG">
<meta property="og:updated_time" content="2019-09-25T06:44:28.275Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="笔记本">
<meta name="twitter:description" content="每天记点小笔记我也可以拿第一">
<meta name="twitter:image" content="http://yoursite.com/2019/02/13/everyday-note/code-cost-longshort.png">





  
  
  <link rel="canonical" href="http://yoursite.com/2019/02/13/everyday-note/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>笔记本 | Xia's Blog</title>
  




  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-142746162-1"></script>
  <script>
    var host = window.location.hostname;
    if (host !== "localhost" || !true) {
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-142746162-1');
    }
  </script>









  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Xia's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于我</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  
    <div class="reading-progress-bar"></div>
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/02/13/everyday-note/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="欢乐一只虾">
      <meta itemprop="description" content="xia写的">
      <meta itemprop="image" content="/uploads/xis.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xia's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">笔记本

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-02-13 15:01:33" itemprop="dateCreated datePublished" datetime="2019-02-13T15:01:33+08:00">2019-02-13</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-09-25 14:44:28" itemprop="dateModified" datetime="2019-09-25T14:44:28+08:00">2019-09-25</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/日常笔记/" itemprop="url" rel="index"><span itemprop="name">日常笔记</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="post-meta-item-icon">
            <i class="fa fa-eye"></i>
             阅读次数： 
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">本文字数：</span>
                
                <span title="本文字数">21k</span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <hr>
<p>每天记点小笔记<br>我也可以拿第一</p>
<hr>
<a id="more"></a>
<h1 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h1><h2 id="2019-09"><a href="#2019-09" class="headerlink" title="2019-09"></a>2019-09</h2><h3 id="2019-09-25"><a href="#2019-09-25" class="headerlink" title="2019-09-25"></a>2019-09-25</h3><blockquote>
<ul>
<li><div title="使减小；使变少；变少">diminish</div></li>
<li><div title="正交的，直角的">orthogonal</div></li>
<li><div title="话虽如此"> That being said </div></li>
<li><div title="本质，精华"> essence </div></li>
<li><div title="初步的"> preliminary </div>


</li>
</ul>
</blockquote>
<ul>
<li><p>The difference between <figure class="highlight plain"><figcaption><span>and ```range``` in Python:</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">- ```range``` returns a Python  ```list``` object</span><br><span class="line">- ```xrange``` returns an ```xrange``` object witch is a ```generators</span><br></pre></td></tr></table></figure></p>
</li>
<li><p>所谓非主流，无非是因为对这个世界不甚了解却又故作深刻的偏执的认知。</p>
</li>
<li><p>paper: Layer Normalization</p>
<ul>
<li>Problem of Batch Normalization:<ul>
<li>the effect is dependent on the mini-batch size</li>
<li>it is not obvious how to apply it to recurrent neural networks.</li>
</ul>
</li>
<li>Compare with Batch Normalization:<ul>
<li>Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity.</li>
<li>Unlike batch normalization, layer normalization performes exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step.</li>
</ul>
</li>
<li>layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. </li>
</ul>
</li>
</ul>
<h3 id="2019-09-24"><a href="#2019-09-24" class="headerlink" title="2019-09-24"></a>2019-09-24</h3><ul>
<li><p>排序算法的稳定性：</p>
<ul>
<li>稳定：冒泡、归并、插入、基数</li>
<li>不稳定：堆排序、快速、直接选择、希尔</li>
</ul>
</li>
<li><p>KNN 和 K-Means</p>
</li>
</ul>
<table width="80%" style="table-layout: fixed; margin:auto"> 
<tr>
<th>KNN</th>
<th>K-Means</th>
</tr>
<tr>
<td>1.分类算法 <br> 2.监督学习</td>
<td>1.聚类算法 <br> 2.非监督算法</td>
</tr>
<tr>
<td>没有明显的训练过程，只需要将所有数据载入到内存中，就可以对新输入的数据进行分类</td>
<td>有明显的训练过程，需要通过不停迭代找出最合适的K个平均值</td>
</tr>
<tr>
<td>K的意思是根据距离新数据最近的K个样本来对新数据进行分类</td>
<td>K的意思是假设所有的数据集中有K个种类，需要初始假设K个均值</td>
</tr>
<tr>
<td colspan="2">相似点：两者都用到了NN（Nearest Neighbor）思想，一般用KD树来实现</td>
</tr>
</table>

<h3 id="2019-09-23"><a href="#2019-09-23" class="headerlink" title="2019-09-23"></a>2019-09-23</h3><ul>
<li>论文笔记：核心思想、预备知识、模型、实验</li>
<li>transformer 中的 Add &amp; Norm 层：<ul>
<li>Add 层类似于残差连接，将自注意力层的输出与自身向量相加，让模型只需要在自注意力层学习增量信息，优点是可以学习到一些很细节的信息。</li>
<li>Norm 层用的是 layer normalization，与 batch normalization 相比优点是针对单样本，不依赖其它数据，可以避免 interval convariate shift。</li>
<li>Feed-Forward Network：两层线性转换和一个激活层（RELU).</li>
</ul>
</li>
</ul>
<h3 id="2019-09-10"><a href="#2019-09-10" class="headerlink" title="2019-09-10"></a>2019-09-10</h3><ul>
<li>Kullback-Leibler Divergence &amp; Entropy<ul>
<li>entropy: it’s a way to quantify exactly how much information is in our data</li>
<li>KL divergence: a way to quantify how much information is lost from true distribution to parameterized approximation.</li>
<li>With KL divergence we can calculate exactly how much information is lost when we approximate one distribution with another.</li>
<li>Divergence not distance. The reason is that KL Divergence is not symmetric.</li>
</ul>
</li>
</ul>
<h3 id="2019-09-09"><a href="#2019-09-09" class="headerlink" title="2019-09-09"></a>2019-09-09</h3><ul>
<li>刘群：基于深度学习的自然语言处理：边界在哪里？<ul>
<li>自然语言处理的范式迁移：从规则，到统计，再到深度学习（我认为深度学习其实只是统计的延伸，严格意义上还是属于统计的范畴）<ul>
<li>人不可能将所有的规则都穷举出来，并且也写不出大量太细的规则。</li>
</ul>
</li>
<li>深度学习解决了自然语言处理哪些问题？<ul>
<li>词语形态问题（morphology）<ul>
<li>在中文中，它体现在词的切分上</li>
<li>在英语等大部分其他语言中则主要体现在形态分析上。</li>
<li>在统计机器翻译时代，有一个比较著名的方法叫做 Factored statistical machine translation （基于要素的翻译方法）：将一个词分成很多要素，然后分别翻译每个要素，最后汇总要素得到结果。</li>
<li>这个问题在神经网络框架下基本不成问题，因为现在机器翻译基本上可以不做分词，大部分中文机器翻译系统基本上基于汉子实现。</li>
</ul>
</li>
<li>句法结构问题<ul>
<li>在神经网络机器翻译框架下，神经网络可以很好的捕捉句子结构，无需进行句法分析，系统可以自动获得处理复杂结构句子的翻译的能力。</li>
</ul>
</li>
<li>多语言问题<ul>
<li>如果使用中间语言来作文多语言翻译系统的互通桥梁，需要开发的系统的数量随翻译语言的数量呈线性增长；否则，开发系统的数量随翻译语言的数量呈平方增长。</li>
<li>在基于统计方法的机器翻译时代，普遍采用Pivot方法，即在两个语言的互译中，先将所有语言翻译成英语，再翻译成另一种语言。以这种方法来实现多语言翻译。但是这样会导致错误传播和性能下降。</li>
</ul>
</li>
<li>联合训练问题<ul>
<li>在统计机器翻译时代，各个模块相互独立，导致错误传播的问题很严重。如果采用联合训练会导致模型的复杂度大大增加。</li>
<li>神经网络机器翻译框架下，端到端训练很容易实现。</li>
</ul>
</li>
</ul>
</li>
<li>还有哪些深度学习尚未解决的自然语言处理问题？<ul>
<li>资源短缺问题：商业系统的训练语料基本上都是数以千万计的，这在很多专业领域根本收集不到如此庞大的数据，尤其是一些小众语言。</li>
<li>可解释性和可信任问题：神经网络由于计算过程都是在高纬度空间中完成，缺乏可解释性，加之神经网络偶尔犯的一些错误会直接给人带来不信任感。</li>
<li>可控制性问题：当神经网络出现一些错误时我们几乎无法通过修改神经网络来修正这些错误。</li>
<li>超长文本问题：虽然最近提出的BERT、GPT等模型将单词可处理的句子长度扩展到了几百到上千字之多，但是长度超过千字的篇章翻译还是无法一次性完成，还有很多问题尚待解决。</li>
<li>缺乏常识问题：举个🌰。“Bank fishing is fishing from places where the land meets the water’s edge.” 谷歌翻译翻译成“银行捕鱼是从陆地与水边相遇的地方捕鱼的。”</li>
</ul>
</li>
<li>基于深度学习的自然语言处理，其边界在哪里？<ul>
<li>数据边界：数不够</li>
<li>语义边界：深度学习只能对字词符号之间的关系进行建模，并不能对所描述的问题语义或者输入的句子语义进行建模。</li>
<li>符号边界：再好的神经网络也只是在符号之间寻找规律，不能准确的定义或者描述任何特定的表达。</li>
<li>因果边界：通过观察数据能得出的结论永远只有关联性而不可能得出因果性。因果性必须有完备的理论和推理支持。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="2019-08"><a href="#2019-08" class="headerlink" title="2019-08"></a>2019-08</h2><h3 id="2019-08-06"><a href="#2019-08-06" class="headerlink" title="2019-08-06"></a>2019-08-06</h3><ul>
<li>Tensorflow<ul>
<li>tf.expand_dims(input, axis)</li>
<li>tf.concat(values, axis)</li>
<li>tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs)</li>
</ul>
</li>
</ul>
<h3 id="2019-08-02"><a href="#2019-08-02" class="headerlink" title="2019-08-02"></a>2019-08-02</h3><blockquote>
<ul>
<li><div title="陡峭的；完全的；垂直的">sheer</div></li>
<li><div title="观点；态度；意见">sentiment</div></li>
<li><div title="粒度">granularity</div></li>
<li><div title=" 不同种类的，多种多样的；混杂的">miscellaneours</div></li>
<li><div title="轶事的；趣闻的">anecdotal</div></li>
<li><div title="气氛；环境">ambience</div></li>
<li><div title="均匀的；平等的">evenly</div>


</li>
</ul>
</blockquote>
<ul>
<li><a href="https://medium.com/dair-ai/adapters-a-compact-and-extensible-transfer-learning-method-for-nlp-6d18c2399f62" target="_blank" rel="noopener">Adapters: A Compact and Extensible Transfer Learning Method for NLP</a><ul>
<li>在 transformer fine-tuning 的时候插入adapter 模块就行微调，原参数不参加训练，只训练adapter和最上层。</li>
</ul>
</li>
</ul>
<h2 id="2019-07"><a href="#2019-07" class="headerlink" title="2019-07"></a>2019-07</h2><h3 id="2019-07-30"><a href="#2019-07-30" class="headerlink" title="2019-07-30"></a>2019-07-30</h3><ul>
<li>哪些参数对于训练BERT模型至关重要（RoBERTa）：<ul>
<li>static masking VS. dynamic masking:<ul>
<li>static masking: 在数据预处理的时候 mask 一次，每次 epoch 中训练实例使用相同的 mask</li>
<li>dynamic masking： 避免让每次 epock 中的训练数据使用相同的 mask，具体方法是：如果一共训练40个epoch，我们就每训练4个 epoch 就重新进行 masking，这样重复10次，意味着每个训练序列都使用相同的 mask 四次。</li>
<li>动态 masking 的效果比静态的好一丢丢。</li>
</ul>
</li>
<li>模型输入格式和下一局预测：<ul>
<li>这里我没有看明白新智元的论文翻译，得去看一下原文。大致意思应该是他们采用没有NSP的完整句子。</li>
</ul>
</li>
<li>扩大 mini-batches, 并提高学习率</li>
</ul>
</li>
</ul>
<h3 id="2019-07-28"><a href="#2019-07-28" class="headerlink" title="2019-07-28"></a>2019-07-28</h3><blockquote>
<ul>
<li><div title="虚构的事/物"> figment </div>

</li>
</ul>
</blockquote>
<ul>
<li>macbook 突然没有声音解决方法： sudo killall coreaudiod 重置音频核心</li>
</ul>
<h3 id="2019-07-24"><a href="#2019-07-24" class="headerlink" title="2019-07-24"></a>2019-07-24</h3><ul>
<li>mac 复制文件地址： <code>option</code>+<code>commend</code>+<code>C</code></li>
<li>markdown文档中加 <code>&amp;nbsp;</code> 可以加空行</li>
</ul>
<h3 id="2019-07-18"><a href="#2019-07-18" class="headerlink" title="2019-07-18"></a>2019-07-18</h3><blockquote>
<ul>
<li><div title="推测；猜想"> conjecture </div>


</li>
</ul>
</blockquote>
<ul>
<li>AutoGraph: 可以将eager execution执行的tensorflow代码以计算图的方式执行，提高执行效率。<ul>
<li>autograph通过<code>contrib.autograph</code>获取</li>
<li>常用方法：<ul>
<li>修饰: <code>@autograph.convert(optional_features=...)</code></li>
<li>autograph.to_code()</li>
<li>autograph.to_graph()</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2019-07-12"><a href="#2019-07-12" class="headerlink" title="2019-07-12"></a>2019-07-12</h3><blockquote>
<ul>
<li><div title="人造的"> synthetic </div></li>
<li><div title="根本地;彻底地;完全地"> redically </div>


</li>
</ul>
</blockquote>
<h3 id="2019-07-07"><a href="#2019-07-07" class="headerlink" title="2019-07-07"></a>2019-07-07</h3><ul>
<li><p>Linux 换源：</p>
<ul>
<li>apt: /etc/apt/sources.list</li>
<li>pip: ~/.pip/pip.conf</li>
<li>conda: ~/.condarc</li>
</ul>
</li>
<li><p>ssh 服务：</p>
<ul>
<li>开启/关闭开机自启：systemctl enable ssh</li>
<li>单次启动/关闭：systemctl start ssh</li>
</ul>
</li>
</ul>
<h3 id="2019-07-05"><a href="#2019-07-05" class="headerlink" title="2019-07-05"></a>2019-07-05</h3><blockquote>
<ul>
<li><div title="忽视；忽略">neglect</div>

</li>
</ul>
</blockquote>
<h3 id="2019-07-04"><a href="#2019-07-04" class="headerlink" title="2019-07-04"></a>2019-07-04</h3><ul>
<li>Batch Normalization<ul>
<li>一般归一化的是激活函数之前的$z^k$，也有researcher觉得应该归一化激活后的$a^k$</li>
</ul>
</li>
</ul>
<ul>
<li>Paper: Batch Normalization<ul>
<li>Fixing the distribution of the layer inputs $x$ as the training progresses to accelerate the training process.</li>
<li>It has been long known (LeCun’s Efficient BackProp) that the network training converges faster if its inputs are whitened, which means linearly transformed to have zero means and unit variance.</li>
<li>A potential problem is that the gradient descent optimization does not take into account the fact that the normalization takes place. Solution: ensure that, for any parameter values, the network always produces activations with the desired distribution.</li>
<li>The Jacobians</li>
</ul>
</li>
</ul>
<h3 id="2019-07-03"><a href="#2019-07-03" class="headerlink" title="2019-07-03"></a>2019-07-03</h3><blockquote>
<ul>
<li><div title="漂白"> whiten </div> </li>
<li><div title="间隔；幕间休息；间距"> interval </div></li>
<li><div title="散布,散置"> intersperse </div>

</li>
</ul>
</blockquote>
<ul>
<li>deeplearning.ai<ul>
<li>bias VS. variance:<ul>
<li>high bias: underfitting</li>
<li>high variance: overfitting</li>
<li>otherwise: good classifier</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2019-07-02"><a href="#2019-07-02" class="headerlink" title="2019-07-02"></a>2019-07-02</h3><blockquote>
<ul>
<li><div title="臭名昭著的；声名狼藉的"> notorious </div></li>
<li><div title="放大；扩大"> amplify </div></li>
<li><div title="补偿；报酬"> compensate </div></li>
<li>convergence VS. divergence</li>
<li><div title="坚固的；大量的；重大的；实质的"> substantial </div>


</li>
</ul>
</blockquote>
<ul>
<li>Paper: Batch Normalization<ul>
<li>Internal covariate shift: the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change.</li>
<li>It allows us to use much higher learning rates and be less careful about initialization.</li>
<li>It also acts as a regularization, in some cases eliminating the need for Dropout. </li>
<li>Stochastic gradient descent (SGD)</li>
<li>Fixed distribution of inputs to a sub-network would have positive consequences for the layers outside the sub-network.</li>
<li>The saturated regime of the nonlinearity. Could be addressed by using Rectified Linear Units, $ReLU(x)=max(x,0)$, careful initialization, small learning rates.</li>
</ul>
</li>
</ul>
<h2 id="2019-06"><a href="#2019-06" class="headerlink" title="2019-06"></a>2019-06</h2><h3 id="2019-06-28"><a href="#2019-06-28" class="headerlink" title="2019-06-28"></a>2019-06-28</h3><ul>
<li>Paper: Advances in Pre-Training Distributed Word Representations:<ul>
<li>standard cbow model<ul>
<li>the objective of it is to maximize the log-likelihood of the probability of the words given their surrounding:<script type="math/tex; mode=display">\sum^{T}_{t=1}\log p(w_t|C_t)</script></li>
<li>scoring function (simply getting average):<script type="math/tex; mode=display">s(w,C)=\frac{1}{|C|}\sum_{w^\prime \in C}u^T_{w^\prime}v_w</script></li>
<li>word subsampling:<script type="math/tex; mode=display">p_{disc}(w) = 1 - \sqrt{\frac{t}{f_w}}</script></li>
</ul>
</li>
<li>position-dependent weighting<ul>
<li>reweight the context vectors</li>
<li>each position $p$ in a context window is associated with a vector $d_p$, which is learnt from training phase.<script type="math/tex; mode=display">v_C=\sum_{p\in P}d_p</script></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2019-06-27"><a href="#2019-06-27" class="headerlink" title="2019-06-27"></a>2019-06-27</h3><blockquote>
<ul>
<li><div title="未察觉...；无视...">be oblivious to</div></li>
<li><div title="混乱；使凌乱">clutter</div></li>
<li><div title="切线；切线的；离题的">tangent</div></li>
<li><div title="...的关键；难题">the crux of</div>



</li>
</ul>
</blockquote>
<h3 id="2019-06-24"><a href="#2019-06-24" class="headerlink" title="2019-06-24"></a>2019-06-24</h3><ul>
<li>hexo next 主题的图片边框： themes/next/source/css/_common/components/post/post-expand.styl。 部署前不要忘记hexo clean。</li>
<li>word frequency distribution, Zipf distribution, which implies that most of the words belongs to small subset of the entire vovabulary <a href="https://ieeexplore.ieee.org/abstract/document/165464" target="_blank" rel="noopener">(Li, 1992)</a>.</li>
</ul>
<h3 id="2019-06-18"><a href="#2019-06-18" class="headerlink" title="2019-06-18"></a>2019-06-18</h3><blockquote>
<ul>
<li><div title="复数；复数的"> plural </div></li>
<li><div title="杂货店"> grocery </div></li>
<li><div title="选民；构成部分"> constituent </div></li>
<li><div title="微妙的；细微的"> subtle </div></li>
<li><div title="离散的"> diffuse </div></li>
<li><div title="减轻；救济；安慰"> relief </div></li>
<li><div title="激烈的；猛烈的"> drastic </div>

</li>
</ul>
</blockquote>
<ul>
<li><a href="https://colah.github.io/posts/2015-09-Visual-Information/" target="_blank" rel="noopener">Information Theory</a><ul>
<li>entropy = optimal average length in coding problem</li>
<li>prefix codes - prefix property: no codeword should be the prefix of another codeword</li>
<li>short codewords reduce the average message length but are expensive, while long codewords increase the average message length but are cheep. <img src="/2019/02/13/everyday-note/code-cost-longshort.png"></li>
<li><img src="/2019/02/13/everyday-note/calculating-entropy.JPG"></li>
<li>The more concentrated the probability, the more people can craft a clever code with short average messages. The more diffuse the probability, the longer people’s messages have to be.<ul>
<li>If there’s two things that could happen with 50% probability, people only need to send 1 bit. </li>
<li>If there’s 64 different things that could happen with equal probability, they would have to send 6 bits. </li>
</ul>
</li>
<li>entropy VS. cross entropy:<ul>
<li>cross entropy - the average length of communicating an event from one distribution with the optimal code for another distribution: <img src="/2019/02/13/everyday-note/cross-entropy.JPG"></li>
<li>cross-entropy isn’t symmetric. The more different the distributions $p$ and $q$ are, the more the cross-entropy of $p/q$ with respect to $q/p$ will be bigger than the entropy of $p/q$: <img src="/2019/02/13/everyday-note/cross-entropy-unsymmetrical.JPG"></li>
<li>This kind of difference is called Kullback-Leibler (KL) divergence: <script type="math/tex; mode=display">D_q(p)=H_q(p) - H(p)</script>KL divergence like a distance between two distributions. It measures how different they are</li>
</ul>
</li>
<li>conditional entropy: <script type="math/tex; mode=display">\begin{split} H(X|Y) &= \sum_{y}p(y)\sum_{x}p(x|y)log_2(\frac{1}{p(x|y)})\\ &=\sum_{x,y}p(x|y)log_2(\frac{1}{p(x|y)})\end{split}</script></li>
</ul>
</li>
</ul>
<h3 id="2019-06-13"><a href="#2019-06-13" class="headerlink" title="2019-06-13"></a>2019-06-13</h3><blockquote>
<ul>
<li>cumbersome</li>
<li>polysemy</li>
<li>as it were</li>
</ul>
</blockquote>
<ul>
<li>two properties of sigmoid function:<script type="math/tex; mode=display">\sigma(-u) = 1-\sigma(u)</script><script type="math/tex; mode=display">\frac{d\sigma(u)}{du} = \sigma(u)\sigma(-u)</script></li>
</ul>
<h3 id="2019-06-11"><a href="#2019-06-11" class="headerlink" title="2019-06-11"></a>2019-06-11</h3><blockquote>
<ul>
<li>derivation</li>
<li>hierarchical</li>
<li>posterior</li>
</ul>
</blockquote>
<ul>
<li>python 中的 StopIteration 异常。</li>
</ul>
<h3 id="2019-06-10"><a href="#2019-06-10" class="headerlink" title="2019-06-10"></a>2019-06-10</h3><blockquote>
<ul>
<li>abound</li>
</ul>
</blockquote>
<ul>
<li>NLP tag:</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>Scheme</th>
<th style="text-align:center">Begin</th>
<th style="text-align:center">Inside</th>
<th style="text-align:center">End</th>
<th style="text-align:center">Single</th>
<th style="text-align:center">Other</th>
</tr>
</thead>
<tbody>
<tr>
<td>IOB</td>
<td style="text-align:center">B-X</td>
<td style="text-align:center">I-X</td>
<td style="text-align:center">I-X</td>
<td style="text-align:center">B-X</td>
<td style="text-align:center">O</td>
</tr>
<tr>
<td>IOE</td>
<td style="text-align:center">I-X</td>
<td style="text-align:center">I-X</td>
<td style="text-align:center">E-X</td>
<td style="text-align:center">E-X</td>
<td style="text-align:center">O</td>
</tr>
<tr>
<td>IOBES</td>
<td style="text-align:center">B-X</td>
<td style="text-align:center">I-X</td>
<td style="text-align:center">E-X</td>
<td style="text-align:center">S-X</td>
<td style="text-align:center">O</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>Activation Funciton:<ul>
<li>can add non-linearity to the output, so it makes neural network solve non-linear problems</li>
<li>type:<ul>
<li>linear</li>
<li>sigmoid</li>
<li>tanh</li>
<li>Rectified Linear Unit (ReLU)</li>
<li>softmax</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2019-06-04"><a href="#2019-06-04" class="headerlink" title="2019-06-04"></a>2019-06-04</h3><blockquote>
<ul>
<li>derivatives</li>
<li>integrals</li>
<li>differential equations</li>
<li>pendulum</li>
</ul>
</blockquote>
<ul>
<li>3Blue1Brown: 微分方程概论-第一章<ul>
<li>Ordinary Differential Equation(ODE) VS. Partial Diffential Equation(PDE)<ul>
<li>PDE: think of them as involving a continum of values changing with time, like the temperature at every point of a solid body</li>
</ul>
</li>
<li>Second Order Differential Equation:<ul>
<li>whih means the highest derivative you find in this expression is a second derivative</li>
<li>higher-order differential equations include derivatives more than two</li>
</ul>
</li>
<li>phase space</li>
</ul>
</li>
</ul>
<h2 id="2019-05"><a href="#2019-05" class="headerlink" title="2019-05"></a>2019-05</h2><h3 id="2019-05-31"><a href="#2019-05-31" class="headerlink" title="2019-05-31"></a>2019-05-31</h3><ul>
<li>修改graph结构：<code>tf.import_graph_def(graph_def,name=&#39;&#39;, input_map={&#39;model_intent/embedding_lookup:0&#39;: char_embed})</code></li>
</ul>
<h3 id="2019-05-29"><a href="#2019-05-29" class="headerlink" title="2019-05-29"></a>2019-05-29</h3><ul>
<li>python 中 write 和 writeline 区别：write只能以字符串为参数，writeline可以传入字符列表。</li>
</ul>
<h3 id="2019-05-29-1"><a href="#2019-05-29-1" class="headerlink" title="2019-05-29"></a>2019-05-29</h3><blockquote>
<ul>
<li>enact</li>
<li>curb</li>
<li>proliferate</li>
</ul>
</blockquote>
<ul>
<li>最小二乘法：将需要预估的参数作为变量，对每个变量求偏导，求每个变量倒数为0的解: <script type="math/tex; mode=display">\hat{\beta}=(X^TX)^{-1}X^Ty</script></li>
</ul>
<h3 id="2019-05-21"><a href="#2019-05-21" class="headerlink" title="2019-05-21"></a>2019-05-21</h3><ul>
<li>在序列标准任务中，逐帧softmax和条件随机场（CRF）的根本不同：<strong>前者将序列标注看成是n个k分类问题，后者将序列标注看成是1个 $k^n$ 分类问题</strong>。</li>
</ul>
<h3 id="2019-05-15"><a href="#2019-05-15" class="headerlink" title="2019-05-15"></a>2019-05-15</h3><blockquote>
<p>生词：</p>
<ul>
<li>imaginary friend</li>
</ul>
</blockquote>
<ul>
<li>Information Theory:<ul>
<li>Simpson’s Paradox  <img src="/2019/02/13/everyday-note/simpson-separated-note.png"></li>
<li>code:<ul>
<li>prefix property: no codeword should be the prefix of another codeword</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2019-05-14"><a href="#2019-05-14" class="headerlink" title="2019-05-14"></a>2019-05-14</h3><blockquote>
<p>生词：</p>
<ul>
<li>toll</li>
<li>optic</li>
<li>retina</li>
<li>stave off</li>
<li>gadget</li>
<li>unerringly</li>
<li>I have <strong>misgiving</strong> about…</li>
<li>intimidate</li>
</ul>
</blockquote>
<h3 id="2019-05-09"><a href="#2019-05-09" class="headerlink" title="2019-05-09"></a>2019-05-09</h3><blockquote>
<p>生词：</p>
<ul>
<li>metabolism</li>
<li>keeps on ticking</li>
<li>glucose</li>
<li>rev up</li>
<li>molecular</li>
<li>carbs</li>
<li>exercise regimen</li>
<li>stunning feat of plasticity</li>
</ul>
</blockquote>
<ul>
<li>词袋模型 bag-of-word</li>
</ul>
<h3 id="2019-05-05"><a href="#2019-05-05" class="headerlink" title="2019-05-05"></a>2019-05-05</h3><ul>
<li>Transformer:<ul>
<li>encoder 最后一层的输出会在decoder每一层被用到。</li>
<li>multi-head attention 层是为了找出tokens之间的关系，之后的add是为了让它不要忘记自己。</li>
<li>the Dot-Product Attention for head $i$ : <script type="math/tex; mode=display">Attention(Q_i,K_i,V_i) = softmax(\frac{Q_iK_i^{T}}{\sqrt[]{d_k}})V_i</script><ul>
<li>其中：<ul>
<li>$Q_i = XW_i^Q$ ，维度为 $input\_length \times d_k$</li>
<li>$K_i = XW_i^K$ ，维度为 $input\_length \times d_k$</li>
<li>$V_i = XW_i^V$ ，维度为 $input\_length \times d_v$</li>
</ul>
</li>
<li>论文中（<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention is all your need</a>）使用$d_k=d_v=\frac{emb\_dim}{h}$</li>
<li>不同 <em>head</em> 不共享$W_i^Q$，$W_i^K$和$W_i^V$，且都是随机初始化。</li>
<li>decoder block 里有两层multi-head attention加一层feed forward，第一层attention用来找出target sequence里的关系（需要用到mask，只允许看对应token的左边的token），第二层attention结合encoder的输出（E）和decoder上一个block的输出（D）,是为了用encoder的input sequence中各个token来表示target sequence中的各个token,相应的：<ul>
<li>$Q_i = DW_i^Q$ ，维度为 $target\_length \times d_k$</li>
<li>$K_i = EW_i^K$ ，维度为 $input\_length \times d_k$</li>
<li>$V_i = EW_i^K$ ，维度为 $input\_length \times d_v$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="2019-04"><a href="#2019-04" class="headerlink" title="2019-04"></a>2019-04</h2><h3 id="2019-04-25"><a href="#2019-04-25" class="headerlink" title="2019-04-25"></a>2019-04-25</h3><blockquote>
<p>生词：</p>
<ul>
<li><strong>diverse</strong> application domains</li>
<li>propose a <strong>taxonomy</strong></li>
</ul>
</blockquote>
<h3 id="2019-04-19"><a href="#2019-04-19" class="headerlink" title="2019-04-19"></a>2019-04-19</h3><blockquote>
<p>生词：</p>
<ul>
<li><strong>granular</strong> cell</li>
<li><strong>scrutinize</strong> over the concept</li>
</ul>
</blockquote>
<ul>
<li>word2vector<ul>
<li>高频词汇采样</li>
<li>负采样: 每次只更新target词和随机几个（5-20）“negative word”对应的权重，减小计算量，加快训练过程。</li>
</ul>
</li>
</ul>
<h3 id="2019-04-17"><a href="#2019-04-17" class="headerlink" title="2019-04-17"></a>2019-04-17</h3><blockquote>
<p>生词：</p>
<ul>
<li>chronic</li>
<li>exceedingly</li>
<li>bound to be exceedingly limited</li>
</ul>
</blockquote>
<ul>
<li>三星手机添加notification提示音：<ul>
<li>在手机文件个目录里新建一个‘Media’文件夹</li>
<li>在‘Media’文件夹里新建‘Notifications’文件夹</li>
<li>在‘Notifications’里添加<code>.MP3</code>文件</li>
</ul>
</li>
</ul>
<h3 id="2019-04-15"><a href="#2019-04-15" class="headerlink" title="2019-04-15"></a>2019-04-15</h3><ul>
<li>Discriminative Model VS. Generative Model<ul>
<li>Discriminative: they model the decision boundary between the different classes, such as Logistic Regression which based on Maximum Likelihood.</li>
<li>Generative model: they model how the data was generated can be used to make classifications, such as Naive Bayes.</li>
</ul>
</li>
<li><a href="https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21?source=email-b01757374e14-1555187113798-digest.reader------1-58------------------811dbe59_37a4_4c19_9db6_ee6b790471a8-1&amp;sectionName=top" target="_blank" rel="noopener">Debugging Neural Networks</a><ol>
<li>Start simple:<ul>
<li>Building a simpler model first. Then gradually add model complexity.</li>
<li>Train your model on a single data point. Using 1 or 2 training data points to confirm whether model is able to overfit, which means it immediately overfit with a training accuracy of 100% and random guessing in validation phase.</li>
</ul>
</li>
<li>Confirm your loss:<ul>
<li>The loss is appropriate for the task(choosing cross-entropy loss or mean squared error?)</li>
<li>Your loss functions are being measured on the correct scale. It’s best to first check the data loss alone (so set regularization strength to zero).</li>
</ul>
</li>
<li>Check intermediate outputs and connections</li>
<li>Diagnose parameters<ul>
<li>Batch size: <ul>
<li>Small batch sizes will result in a learning process that converges quickly at the cost of noise in the training process and might lead to optimization difficulties.</li>
<li>Large-batch methods tend to converge to sharp minimizers of the training and testing functions-and as is well known, sharp minima lead to poorer generalization.</li>
</ul>
</li>
<li>Learning rate: Consider incorporating learning rate scheduling to decrease the learning rate as training progresses</li>
<li>Gradient clipping: Useful for addressing any exploding gradients.</li>
<li>Batch normalization: Fight the internal covariate shift problem.</li>
<li>Stochastic Gradient Descent (SGD): A recommended starting point is Adam or plain SGD with Nesterov momentum.</li>
<li>Regularization: </li>
<li>Dropout</li>
</ul>
</li>
<li>Track your work</li>
</ol>
</li>
</ul>
<h3 id="2019-04-11"><a href="#2019-04-11" class="headerlink" title="2019-04-11"></a>2019-04-11</h3><blockquote>
<p>生词：</p>
<ul>
<li>get accustomed to</li>
<li>take on</li>
<li>leverage</li>
<li>prominent</li>
<li>autonomy</li>
</ul>
</blockquote>
<h3 id="2019-04-09"><a href="#2019-04-09" class="headerlink" title="2019-04-09"></a>2019-04-09</h3><blockquote>
<p>生词：</p>
<ul>
<li>unary</li>
<li>sophisticated</li>
<li>prevalent</li>
<li>latent</li>
</ul>
</blockquote>
<ul>
<li>Transfer Learning:<ul>
<li>transfer learning is the process of trainibg a model on a large-scale dataset and then using that pretrained model to conduct learning for another downstream task.</li>
<li>recent work:<ul>
<li><a href="https://arxiv.org/abs/1801.06146" target="_blank" rel="noopener">ULMFit</a></li>
<li><a href="https://allennlp.org/elmo" target="_blank" rel="noopener">ELMo</a>: 2 layers of LSTM</li>
<li><a href="https://arxiv.org/abs/1806.05662" target="_blank" rel="noopener">GLoMo</a></li>
<li><a href="https://blog.openai.com/language-unsupervised/" target="_blank" rel="noopener">OpenAI transformer</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2019-04-04"><a href="#2019-04-04" class="headerlink" title="2019-04-04"></a>2019-04-04</h3><blockquote>
<p>生词：</p>
<ul>
<li>cope with</li>
<li>be composed of</li>
<li>sanitary</li>
<li>sock</li>
<li>disinfectant</li>
<li>armrest</li>
<li>fungal</li>
</ul>
</blockquote>
<h3 id="2019-04-03"><a href="#2019-04-03" class="headerlink" title="2019-04-03"></a>2019-04-03</h3><blockquote>
<p>生词：</p>
<ul>
<li>have a <strong>self-consciousness</strong></li>
<li>bungling goon</li>
<li>play is <strong>innate</strong></li>
<li>for the sake of</li>
<li><strong>befuddled</strong> aunt/uncle/parent</li>
<li><strong>enamored</strong> with face</li>
<li>recess</li>
<li>paramount</li>
<li>wane</li>
<li>sophisticate</li>
</ul>
</blockquote>
<ul>
<li>How to interact with children:<ul>
<li>Ages 0-1:<ul>
<li>attraction to faces (expressions) only grows from there</li>
<li>speak as much as possible using baby talk with infant</li>
</ul>
</li>
<li>Ages 1-3:<ul>
<li>wait for an invitation into the child’s world. Follow their verbal and nonverbal cues.</li>
</ul>
</li>
<li>Ages 3-5:<ul>
<li>don’t push child to learning school readiness and other skills.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2019-04-02"><a href="#2019-04-02" class="headerlink" title="2019-04-02"></a>2019-04-02</h3><blockquote>
<p>生词：</p>
<ul>
<li>obsessive-compulsive disorder(OCD)</li>
</ul>
</blockquote>
<h2 id="2019-03"><a href="#2019-03" class="headerlink" title="2019-03"></a>2019-03</h2><h3 id="2019-03-30"><a href="#2019-03-30" class="headerlink" title="2019-03-30"></a>2019-03-30</h3><!-- more -->
<blockquote>
<p>生词:</p>
<ul>
<li>dodge</li>
<li>legitimate</li>
<li>folk</li>
<li>profitable</li>
</ul>
</blockquote>
<h3 id="2019-03-28"><a href="#2019-03-28" class="headerlink" title="2019-03-28"></a>2019-03-28</h3><blockquote>
<p>生词：</p>
<ul>
<li>grasp</li>
<li>slang</li>
</ul>
</blockquote>
<ul>
<li><a href="https://www.tensorflow.org/tutorials/representation/word2vec" target="_blank" rel="noopener">Word2vec</a>：<ul>
<li>有两种类型<ul>
<li>连续词袋模型（CBOW）：根据上下文预测目标字词</li>
<li>Skip-Gram：根据从目标字词中预测上下文字词</li>
</ul>
</li>
<li>噪声对比估算（NCE）损失是基于逻辑回归模型进行定义的</li>
</ul>
</li>
<li><a href="https://nlp.stanford.edu/projects/glove/" target="_blank" rel="noopener">GloVe</a>:<ul>
<li>先从文本中处理得到一个word-word co-occurrence矩阵 <img src="/2019/02/13/everyday-note/word-word-co-occurrence-matrix.png" title="word-word co-occurrence矩阵的一个小栗子"></li>
<li>训练过程是让词向量的点乘结果尽可能等于相应两个词一起出现的概率的对数。</li>
</ul>
</li>
</ul>
<h3 id="2019-03-26"><a href="#2019-03-26" class="headerlink" title="2019-03-26"></a>2019-03-26</h3><blockquote>
<p>生词：</p>
<ul>
<li>meteor</li>
<li>splash</li>
<li>bruise</li>
<li>shatter</li>
<li>flock </li>
<li>patio</li>
<li>bureaucratic</li>
<li>unlikely</li>
<li>trickle</li>
<li>deficiency</li>
<li>patriotic</li>
</ul>
</blockquote>
<ul>
<li>vim:<ul>
<li>v: 可视模式</li>
<li>w/b：前一个单词/后一个单词</li>
<li>^：本行第一个不是blank字符的位置</li>
<li>0：本行第一个位置</li>
<li>$：本行行尾</li>
<li>g_：本行最后一个不是blank字符的位置</li>
<li>p：黏贴</li>
<li>u：undo</li>
<li>CTRL-r：redo</li>
<li>d: 删除并复制</li>
<li>y：直接复制</li>
</ul>
</li>
</ul>
<h3 id="2019-03-21"><a href="#2019-03-21" class="headerlink" title="2019-03-21"></a>2019-03-21</h3><ul>
<li>从远程Git库中下在分支到本地：<ul>
<li><code>git pull origin branch_name:local_name</code> ：获取远程库中的branch_name分支与本地的local_name分支进行merge，如果要与本地当前分支进行merge可以不写冒号和冒号后面内容，如果本地所在的当前分支（dev）与对应远程分支（origin/dev）已建立联系，只需 <code>git pull origin</code></li>
<li><code>git checkout -b local_name origin/branch_name</code>: 自动创建一个新的本地分支（local_name），并与指定远程分支关联起来。</li>
</ul>
</li>
</ul>
<h3 id="2019-03-19"><a href="#2019-03-19" class="headerlink" title="2019-03-19"></a>2019-03-19</h3><ul>
<li>支持向量机 (Support Vector Machine)：<ul>
<li>支持向量 (support vectors):<ul>
<li>support vectores are the samples that are most difficult to classify</li>
<li>they directly affect the process to find the optimum location of the decision boundaries.</li>
</ul>
</li>
<li>constrained optimization - Lagrange Multipliers<ul>
<li>obtaining extrema of function $f(x,y)$ under the constraint $g(x,y) = A$ -&gt; $f^\prime(x,y) = \lambda g^\prime(x,y)$:</li>
</ul>
</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">\begin{split}
f^\prime (x,y) &= [f(x,y) + \lambda(A-g(x,y))]^\prime\\
 &=f^\prime(x,y) - \lambda g^\prime(x,y) = 0
\end{split} \tag{19.03.19.1}</script><ul>
<li>Math in Markdown<ul>
<li>one expression in multi-lines : \begin{split} align by ‘&amp;’ before ‘=’</li>
<li>multi-expressions in multi-lines : \begin{eqnarray*} align by ‘&amp;…&amp;’ wrap ‘=’</li>
</ul>
</li>
</ul>
<h3 id="2019-03-06"><a href="#2019-03-06" class="headerlink" title="2019-03-06"></a>2019-03-06</h3><blockquote>
<p>生词：</p>
<ul>
<li>treadmill</li>
<li>stationary</li>
<li>rack up</li>
<li>bold</li>
<li>obligation</li>
</ul>
</blockquote>
<h3 id="2019-03-05"><a href="#2019-03-05" class="headerlink" title="2019-03-05"></a>2019-03-05</h3><ul>
<li>Conda 操作：<ul>
<li>信息： <code>conda info</code></li>
<li>更新自己： <code>conda update conda</code></li>
<li>安装/更新 包：<code>conda install/update [package]</code></li>
<li>创建一个新python环境：<code>conda create --name mingzi python=3.6</code></li>
<li>激活一个python环境：<code>activate mingzi</code> (Win)/<code>source activate mingzi</code> (Linux/mac)</li>
<li>取消激活当前环境：<code>deactivate</code> (Win)/ <code>source deactivate</code> (Linux/mac)</li>
<li>环境列表：<code>conda env list</code></li>
</ul>
</li>
</ul>
<h3 id="2019-03-04"><a href="#2019-03-04" class="headerlink" title="2019-03-04"></a>2019-03-04</h3><ul>
<li>Vim 操作：<ul>
<li>dd -&gt; 删除当前行，并将这行保存到剪切板里</li>
<li>p -&gt; 粘贴剪切板里的内容</li>
<li>h，j，k，l -&gt; 移动光标（方向键也可以）</li>
<li>：help \<command> -&gt;  显示相关命令的帮助</li>
<li>u -&gt; 撤销（undo）</li>
</ul>
</li>
</ul>
<h2 id="2019-02"><a href="#2019-02" class="headerlink" title="2019-02"></a>2019-02</h2><h3 id="2019-02-28"><a href="#2019-02-28" class="headerlink" title="2019-02-28"></a>2019-02-28</h3><blockquote>
<p>生词：</p>
<ul>
<li>arithmetic</li>
</ul>
</blockquote>
<ul>
<li>Attention mechanism:<ul>
<li>attention 计算时(softmax之前)，每个待加权部分（$y_i$）的权重只跟上下文（context）$C$ 有关，与别的部分（$y_j, j \neq i$）无关。如下图： <img src="/2019/02/13/everyday-note/attention.JPG"></li>
<li>Soft Attention vs. Hard Attention<ul>
<li>soft attention: 结果为各个待加权部分的加权和</li>
<li>hard attention：结果为根据相应概率随机选取的一个待加权部分</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2019-02-19"><a href="#2019-02-19" class="headerlink" title="2019-02-19"></a>2019-02-19</h3><blockquote>
<p>生词：</p>
<ul>
<li>tickle</li>
<li>sibling</li>
<li>stroke a cat</li>
<li>discrepancy</li>
<li>procrastinate</li>
<li>prone</li>
</ul>
</blockquote>
<ul>
<li>scp命令：<code>scp test.txt  account@ip:/home/xxx/test.txt</code> </li>
</ul>
<h3 id="2019-02-17"><a href="#2019-02-17" class="headerlink" title="2019-02-17"></a>2019-02-17</h3><blockquote>
<p>生词：</p>
<ul>
<li>anecdote</li>
<li>instill</li>
<li>ostracism/banishment</li>
<li>squeak</li>
</ul>
</blockquote>
<ul>
<li>向量叉乘（cross product）， 如果逆时针方向相乘则结果为正，反之结果为负。叉乘的结果（三维空间）为一个向量，这个向量的大小是两个相乘的向量所构成的平行四边形的面积，方向与该平行四边形垂直。求解两个向量的叉乘可以用求解对应行列式的方法（加上一个向量 ($\hat{i},\hat{j}, \hat{k}$)）</li>
<li>线性的严格定义（Formal definition of linearity）:<ul>
<li>可加性： $L(\vec{v}+\vec{w})=L(\vec{v})+L(\vec{w})$</li>
<li>成比性：$L(c\vec{v})=cL(\vec{v})$</li>
</ul>
</li>
<li>因此，函数只要满足上面的规定也是线性的。</li>
<li>The learning your do learning forward can be substantially more efficient if you have all the right intuitions in places.</li>
</ul>
<h3 id="2019-02-15"><a href="#2019-02-15" class="headerlink" title="2019-02-15"></a>2019-02-15</h3><blockquote>
<p>生词：</p>
<ul>
<li>perusal</li>
<li>connotation</li>
<li>fumble</li>
<li>clumsily</li>
<li>intercept</li>
<li>distort</li>
<li>perception</li>
<li>clutter</li>
<li>duality</li>
</ul>
</blockquote>
<ul>
<li><code>GraphDef</code> 是TensorFlow用来保存/加载<code>Graph</code>时的中间对象。保存时，可以通过<code>Graph.as_graph_def()</code>或者<code>Session.graph_def</code>获取到<code>GraphDef</code>对象用于序列化保存到文件；加载时，也需要先用<code>tf.GraphDef()</code>获取一个空<code>GraphDef</code>对象然后用 <strong>.pb</strong> 文件中的数据来填充（<code>graph_def.ParseFromString(f.read())</code>）,最后通过<code>tf.import_graph_def(graph_def, name=&#39;prefix&#39;)</code>的形式加载到指定的Graph中。</li>
<li>对偶性： 两种数学事物之间自然而又出乎意料的对应关系</li>
</ul>
<h3 id="2019-02-14"><a href="#2019-02-14" class="headerlink" title="2019-02-14"></a>2019-02-14</h3><blockquote>
<p>生词：</p>
<ul>
<li>spouse</li>
<li>tepastry</li>
<li>gargantuan</li>
</ul>
</blockquote>
<ul>
<li><p>用 <code>tf.train.Saver()</code> 保存/载入模型：</p>
<ul>
<li><p><strong>.data</strong> 文件保存权重（weights）；<strong>.meta</strong> 文件保存计算图的各种元数据（metadata）比如学习率和优化器； <strong>.index</strong> 文件保存键-值对，用来根据模型中的tensor name在 .data 文件中找到相应的data。</p>
<ul>
<li><strong>载入图结构和元数据</strong> 用 <code>tf.train.import_meta_graph(&#39;models/model.ckpt-1000.meta&#39;)</code>, 该方法自动将 “models/model.ckpt-1000.meta” 文件中保存的图加载到默认图（default_graph）中并返回一个 <code>Saver</code>。</li>
<li><strong>回复/载入权重（weights）</strong> 的过程跟载入图结构和元数据不同的是它必须在要给 <code>Session</code> 中执行，可以把它理解为权重的初始化类似的过程，如下：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment">#用载入权重的方式来初始化权重，就不需要tf.global_variables_initializer()过程了</span></span><br><span class="line">    saver.restore(sess, <span class="string">'models/model.ckpt.data-1000-00000-of-00001'</span>)</span><br><span class="line">    print(sess.run(global_step_tensor))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>可以声明多个 <code>Saver</code> 来保存所有或特定变量（<code>Variables</code>）并用字典({name:Var})的形式给变量指定名字：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">v1 = tf.Variable(<span class="number">1.</span>, name=<span class="string">"v1"</span>)</span><br><span class="line">all_saver = tf.train.Saver()</span><br><span class="line">v1_saver = tf.train.Saver(&#123;<span class="string">"V1"</span>:v1&#125;)</span><br></pre></td></tr></table></figure>
</li>
<li><p>一个 <code>Saver</code> 只能保存当前 <code>Session</code> 对应 <code>Graph</code> 中的变量,所以程序中有多个 <code>Graph</code> 时要注意 <code>Saver</code> 的使用，下面这样使用 <code>Saver</code> 就会报错：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">g = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    v1 = tf.Variable(<span class="number">5.</span>, dtype=tf.float32)</span><br><span class="line">print(v1.graph == g) <span class="comment"># 输出True</span></span><br><span class="line">print(v1.graph == tf.get_default_graph()) <span class="comment"># 输出False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个Saver如果在绑定默认default_graph的Session中执行会报错，因为v1是在 g 中的变量而不是default_graph中</span></span><br><span class="line">saver = tf.train.Saver([v1])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    saver.save(sess, <span class="string">'./test'</span>) <span class="comment">#执行这句代码时会报错</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>用 <code>tf.saved_model.simple_save</code> 保存模型：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">simple_save(session,</span><br><span class="line">            export_dir,</span><br><span class="line">            inputs=&#123;<span class="string">"x"</span>: x, <span class="string">"y"</span>: y&#125;),</span><br><span class="line">            outputs=&#123;<span class="string">"z"</span>: z&#125;)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>.pb</strong> 文件保存图结构和元数据； <strong>variables</strong> 文件夹内保存当前的权重。</li>
<li>加载saved_model：  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export_dir = ...</span><br><span class="line">...</span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=tf.Graph()) <span class="keyword">as</span> sess:</span><br><span class="line">    tf.saved_model.loader.load(sess, [tag_constants.TRAINING], export_dir)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>把模型的所有信息（包括图数据和权重信息）全部保存到一个 .pb 文件中：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.python.framework <span class="keyword">import</span> graph_util</span><br><span class="line">output_graph_def = graph_util.convert_variables_to_constants(session, </span><br><span class="line">                                                             session.graph_def,</span><br><span class="line">                                                             output_node_names=[</span><br><span class="line">                                                                         <span class="string">'model_intent/intent_scores'</span>])</span><br><span class="line"><span class="keyword">with</span> tf.gfile.FastGFile(<span class="string">'models/intent_model.pb'</span>, mode=<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(output_graph_def.SerializeToString())</span><br></pre></td></tr></table></figure>
<p>  加载保存的 .pb 文件：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_graph</span><span class="params">(frozen_graph_filename)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.gfile.GFile(frozen_graph_filename, <span class="string">"rb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        graph_def = tf.GraphDef()              <span class="comment"># 创建一个空的graph_def</span></span><br><span class="line">        graph_def.ParseFromString(f.read())    <span class="comment"># 用 .pb 文件中保存的对象填充该graph_def</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Graph().as_default() <span class="keyword">as</span> graph:</span><br><span class="line">        <span class="comment"># 下面的name参数的值会作为graph中所有op的前缀</span></span><br><span class="line">        tf.import_graph_def(graph_def, name=<span class="string">"prefix"</span>)</span><br><span class="line">    <span class="keyword">return</span> graph</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="2019-02-13"><a href="#2019-02-13" class="headerlink" title="2019-02-13"></a>2019-02-13</h3><ul>
<li>线性变换（Linear transformation）就是输入一个向量然后输出另一个向量。这里的transformation(变换)和function（函数）同义，之所以用transformation是因为线性代数里面的线性变换是可以在坐标系中（低维）可视化表现出来的。前面的形容词Linear的意思是：整个坐标系变换之后直线依旧是直线，原点保持固定。下面两个变换都不是线性变换(灰线表示原坐标系)。</li>
</ul>
<img src="/2019/02/13/everyday-note/unlinear.JPG"> <img src="/2019/02/13/everyday-note/unlinear1.JPG">
<ul>
<li>矩阵乘法相当于两个线性变换的复合。类似：<img src="/2019/02/13/everyday-note/Composition.JPG"> 线性变换的行列式 可以理解为线性变换后原单位面积被拉伸/压缩的倍数。 用这个概念很好理解：<script type="math/tex; mode=display">det(M_1M_2)=det(M_1)det(M_2)</script><strong>矩阵的逆</strong> 在transformation上的变现形式就是逆变换（顺时针旋转90度对应的矩阵的逆就是逆时针旋转90度对应的矩阵），因此矩阵A乘以矩阵A的逆表示变换之后又逆变换，相当于什么都没做.</li>
<li>矩阵（或者说线性变换）的 秩（rank）可以理解为线性变换结束后整个坐标系的维度（维度是否被压缩，或者保持原维度）。更加严谨的定义是矩阵列空间的维度（换句话说就是所有列向量可以在几维空间里表示）。所以顾名思义，满秩，表示任何两个列向量都不线性相关即秩数等于列向量的个数即达到最大值。</li>
<li>零空间（Null space），一些变换后落在零向量上的向量构成的空间。</li>
</ul>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/07/08/MachineLearning-ZZH/" rel="next" title="读书笔记：《机器学习》by周志华 (阅读中...)">
                <i class="fa fa-chevron-left"></i> 读书笔记：《机器学习》by周志华 (阅读中...)
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/06/23/visual-information-theory/" rel="prev" title="看图理解信息理论中的“熵”、“交叉熵”等概念">
                看图理解信息理论中的“熵”、“交叉熵”等概念 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/uploads/xis.jpg" alt="欢乐一只虾">
            
              <p class="site-author-name" itemprop="name">欢乐一只虾</p>
              <div class="site-description motion-element" itemprop="description">xia写的</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">5</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">4</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">13</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#2019"><span class="nav-text">2019</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2019-09"><span class="nav-text">2019-09</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-09-25"><span class="nav-text">2019-09-25</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-09-24"><span class="nav-text">2019-09-24</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-09-23"><span class="nav-text">2019-09-23</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-09-10"><span class="nav-text">2019-09-10</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-09-09"><span class="nav-text">2019-09-09</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019-08"><span class="nav-text">2019-08</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-08-06"><span class="nav-text">2019-08-06</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-08-02"><span class="nav-text">2019-08-02</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019-07"><span class="nav-text">2019-07</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-07-30"><span class="nav-text">2019-07-30</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-07-28"><span class="nav-text">2019-07-28</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-07-24"><span class="nav-text">2019-07-24</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-07-18"><span class="nav-text">2019-07-18</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-07-12"><span class="nav-text">2019-07-12</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-07-07"><span class="nav-text">2019-07-07</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-07-05"><span class="nav-text">2019-07-05</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-07-04"><span class="nav-text">2019-07-04</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-07-03"><span class="nav-text">2019-07-03</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-07-02"><span class="nav-text">2019-07-02</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019-06"><span class="nav-text">2019-06</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-06-28"><span class="nav-text">2019-06-28</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-06-27"><span class="nav-text">2019-06-27</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-06-24"><span class="nav-text">2019-06-24</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-06-18"><span class="nav-text">2019-06-18</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-06-13"><span class="nav-text">2019-06-13</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-06-11"><span class="nav-text">2019-06-11</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-06-10"><span class="nav-text">2019-06-10</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-06-04"><span class="nav-text">2019-06-04</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019-05"><span class="nav-text">2019-05</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-05-31"><span class="nav-text">2019-05-31</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-05-29"><span class="nav-text">2019-05-29</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-05-29-1"><span class="nav-text">2019-05-29</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-05-21"><span class="nav-text">2019-05-21</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-05-15"><span class="nav-text">2019-05-15</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-05-14"><span class="nav-text">2019-05-14</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-05-09"><span class="nav-text">2019-05-09</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-05-05"><span class="nav-text">2019-05-05</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019-04"><span class="nav-text">2019-04</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-04-25"><span class="nav-text">2019-04-25</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-04-19"><span class="nav-text">2019-04-19</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-04-17"><span class="nav-text">2019-04-17</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-04-15"><span class="nav-text">2019-04-15</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-04-11"><span class="nav-text">2019-04-11</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-04-09"><span class="nav-text">2019-04-09</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-04-04"><span class="nav-text">2019-04-04</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-04-03"><span class="nav-text">2019-04-03</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-04-02"><span class="nav-text">2019-04-02</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019-03"><span class="nav-text">2019-03</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-03-30"><span class="nav-text">2019-03-30</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-03-28"><span class="nav-text">2019-03-28</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-03-26"><span class="nav-text">2019-03-26</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-03-21"><span class="nav-text">2019-03-21</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-03-19"><span class="nav-text">2019-03-19</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-03-06"><span class="nav-text">2019-03-06</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-03-05"><span class="nav-text">2019-03-05</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-03-04"><span class="nav-text">2019-03-04</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2019-02"><span class="nav-text">2019-02</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-02-28"><span class="nav-text">2019-02-28</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-02-19"><span class="nav-text">2019-02-19</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-02-17"><span class="nav-text">2019-02-17</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-02-15"><span class="nav-text">2019-02-15</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-02-14"><span class="nav-text">2019-02-14</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2019-02-13"><span class="nav-text">2019-02-13</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">欢乐一只虾</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
    <span title="站点总字数">46k</span>
  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a></div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="post-meta-divider">|</span>
  

  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>












  













  



  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script src="/lib/fancybox/source/jquery.fancybox.pack.js"></script>

  
  <script src="/lib/reading_progress/reading_progress.js"></script>


  


  <script src="/js/utils.js?v=7.1.2"></script>

  <script src="/js/motion.js?v=7.1.2"></script>



  
  


  <script src="/js/affix.js?v=7.1.2"></script>

  <script src="/js/schemes/pisces.js?v=7.1.2"></script>




  
  <script src="/js/scrollspy.js?v=7.1.2"></script>
<script src="/js/post-details.js?v=7.1.2"></script>



  


  <script src="/js/next-boot.js?v=7.1.2"></script>


  
  <script src="/js/js.cookie.js?v=7.1.2"></script>
  <script src="/js/scroll-cookie.js?v=7.1.2"></script>


  

  
  
  

  

  
  
  


  


  


  




  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
